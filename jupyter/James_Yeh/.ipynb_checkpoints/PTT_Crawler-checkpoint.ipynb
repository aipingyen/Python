{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "from lib.toolbox import gen_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>MoneyDJ人氣指數    XHR=>VC=>Request Payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'09': '59:49',\n",
       " '2017': '',\n",
       " '26': '',\n",
       " 'Apr': '',\n",
       " 'Cache-Control': 'private',\n",
       " 'Content-Length': '84',\n",
       " 'Content-Type': 'application/json',\n",
       " 'Date': 'Wed,',\n",
       " 'GMT': ''}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = gen_header(\"\"\"Cache-Control:private\n",
    "Content-Length:84\n",
    "Content-Type:application/json\n",
    "Date:Wed, 26 Apr 2017 09:59:49 GMT\"\"\")\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"counts\":[{\"count\":4503,\"guid\":\"a180a15b-9e4f-4575-b28f-927fcb5c63a3\",\"svc\":\"NV\"}]}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://www.moneydj.com/InfoSvc/apis/vc'\n",
    "payload='{\"counts\":[{\"svc\":\"NV\",\"guid\":\"a180a15b-9e4f-4575-b28f-927fcb5c63a3\"}]}'\n",
    "head ={ 'Content-Type' :'application/json'}\n",
    "res = r.post(url, data=payload, headers = header)\n",
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"counts\":[{\"count\":4503,\"guid\":\"a180a15b-9e4f-4575-b28f-927fcb5c63a3\",\"svc\":\"NV\"}]}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "soup.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>regEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pat='abc'\n",
    "string='123 abc'\n",
    "match = re.search(pat,string).group()\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article_crawler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-527f27574347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#  Get article contents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlink_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0marticle_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_crawler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# show results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'article_crawler' is not defined"
     ]
    }
   ],
   "source": [
    "# \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "link_list = []\n",
    "article_list = []\n",
    "\n",
    "HOST = 'https://www.ptt.cc/'\n",
    "URL = HOST + 'bbs/Stock/index.html'\n",
    "res = requests.get(URL)\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "# Get total pages   \n",
    "page = int(re.search(r'\\d+',soup.select('a.btn.wide')[1]['href']).group())+1\n",
    "# page to crawl\n",
    "page_to_crawl = 2   \n",
    "\n",
    "# Get links\n",
    "for num in range(page, page - page_to_crawl, -1):\n",
    "    url = HOST + \"bbs/Stock/index{}.html\".format(num)\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text,'lxml')\n",
    "    links = soup.select('div.title > a')\n",
    "    for link in links:\n",
    "        link_list.append(HOST + link['href'])\n",
    "\n",
    "#  Get article contents\n",
    "for url in link_list:\n",
    "    article_list.append(article_crawler(url))\n",
    "\n",
    "# show results\n",
    "article_list[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_crawler(url):\n",
    "    article_dict = {}\n",
    "    article_dict['url'] = url\n",
    "    res2 = requests.get(url)\n",
    "    soup2 =  BeautifulSoup(res2.text, 'lxml')\n",
    "\n",
    "    # handling artical-metaline author title board\n",
    "    value_tags = soup2.select('span.article-meta-value')\n",
    "    values = [val.text for val in value_tags]\n",
    "    article_dict = {k:v for k,v in zip(['author','board','title','time'], values)}\n",
    "    \n",
    "    # finding ip\n",
    "    span2 = soup2.select('span.f2')\n",
    "    for span in span2:\n",
    "        match = re.search('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', span.text.strip())\n",
    "        if match:\n",
    "           article_dict['ip'] = match.group()\n",
    "            \n",
    "    # finding push (type, content, tm)\n",
    "    push_list = []\n",
    "    push_dict = {}\n",
    "    pushs = soup2.select('div.push')\n",
    "    \n",
    "    for push in pushs:\n",
    "        push_dict = {}\n",
    "        push_dict['type'] = push.select_one('.push-tag').text\n",
    "        push_dict['content'] = push.select_one('.push-content').text.replace(':','').strip()\n",
    "        push_dict['tm'] = push.select_one('.push-ipdatetime').text.strip()\n",
    "        push_list.append(push_dict)\n",
    "        \n",
    "    article_dict['push'] = push_list\n",
    "    \n",
    "    \n",
    "    #finding content\n",
    "    main_content = soup2.select_one('#main-content')\n",
    "\n",
    "    for div in main_content.find_all('div'):\n",
    "        div.decompose()\n",
    "    for span in main_content.find_all('span'):\n",
    "        span.decompose()\n",
    "\n",
    "    #Counter() \n",
    "    from collections import Counter\n",
    "    wc = Counter()\n",
    "    for ind in push_list:\n",
    "        if ind['type'] in wc:\n",
    "            wc[ind['type']] += 1\n",
    "        else:\n",
    "            wc[ind['type']] = 1\n",
    "    article_dict['account'] = wc.most_common()\n",
    "    \n",
    "\n",
    "    article_dict['content'] = ''.join(main_content.text.split())\n",
    "    \n",
    "    return article_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "url = 'https://www.ptt.cc/bbs/Stock/M.1493182198.A.89C.html'\n",
    "article_dict = {}\n",
    "article_dict['url'] = url\n",
    "res2 = requests.get(url)\n",
    "soup2 =  BeautifulSoup(res2.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"article-meta-value\">ARKUE ()</span>,\n",
       " <span class=\"article-meta-value\">Stock</span>,\n",
       " <span class=\"article-meta-value\">Re: [標的] 3428 光耀科</span>,\n",
       " <span class=\"article-meta-value\">Wed Apr 26 12:49:55 2017</span>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "value_tags = soup2.select('span.article-meta-value')\n",
    "value_tags\n",
    "# values = [val.text for val in value_tags]\n",
    "# values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARKUE ()', 'Stock', 'Re: [標的] 3428 光耀科', 'Wed Apr 26 12:49:55 2017']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [val.text for val in value_tags]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'ARKUE ()',\n",
       " 'board': 'Stock',\n",
       " 'time': 'Wed Apr 26 12:49:55 2017',\n",
       " 'title': 'Re: [標的] 3428 光耀科'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_dict = {k:v for k,v in zip(['author','board','title','time'], values)}\n",
    "article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"f2\">※ 引述《ARKUE ()》之銘言：\n",
       " </span>, <span class=\"f2\">※ 發信站: 批踢踢實業坊(ptt.cc), 來自: 111.82.86.24\n",
       " </span>, <span class=\"f2\">※ 文章網址: <a href=\"https://www.ptt.cc/bbs/Stock/M.1493182198.A.89C.html\" rel=\"nofollow\" target=\"_blank\">https://www.ptt.cc/bbs/Stock/M.1493182198.A.89C.html</a>\n",
       " </span>, <span class=\"f2\">※ 編輯: ARKUE (111.82.86.24), 04/26/2017 12:51:59\n",
       " </span>, <span class=\"f2\">※ 編輯: ARKUE (111.82.86.24), 04/26/2017 12:58:51\n",
       " </span>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2 =  BeautifulSoup(res2.text, 'lxml')\n",
    "# finding ip\n",
    "span2 = soup2.select('span.f2')\n",
    "span2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'111.82.86.24'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2 =  BeautifulSoup(res2.text, 'lxml')\n",
    "# finding ip\n",
    "span2 = soup2.select('span.f2')\n",
    "for span in span2:\n",
    "    match = re.search('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', span.text.strip())\n",
    "                    #\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}  1~3\n",
    "if match:\n",
    "    article_dict['ip'] = match.group()\n",
    "article_dict['ip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '今天51區還在喊盤，這隻完了', 'tm': '04/26 12:54', 'type': '推 '},\n",
       " {'content': '我不是跟投顧做單的～是自己判斷的', 'tm': '04/26 12:54', 'type': '→ '},\n",
       " {'content': '融資15%...籌碼很亂，小心為上', 'tm': '04/26 12:55', 'type': '→ '},\n",
       " {'content': '有賺先跑比較好', 'tm': '04/26 12:55', 'type': '→ '},\n",
       " {'content': '哈我今天空單也了結了 轉多', 'tm': '04/26 12:56', 'type': '推 '},\n",
       " {'content': '雷大概念股', 'tm': '04/26 13:03', 'type': '推 '},\n",
       " {'content': '1500多張的融券 補起來也是會很猛的', 'tm': '04/26 13:05', 'type': '→ '},\n",
       " {'content': '跟上光學族群 這隻會飆喔', 'tm': '04/26 13:06', 'type': '推 '},\n",
       " {'content': '另外提醒一下 這檔的特色就是五檔跳動很快 一不小', 'tm': '04/26 13:06', 'type': '→ '},\n",
       " {'content': '心 就是大漲大跌', 'tm': '04/26 13:06', 'type': '→ '},\n",
       " {'content': '你是誠懇哥的粉絲齁', 'tm': '04/26 13:14', 'type': '推 '},\n",
       " {'content': '這支昨天吃筍出一半，好弱', 'tm': '04/26 13:14', 'type': '噓 '},\n",
       " {'content': '要噴噴了', 'tm': '04/26 13:28', 'type': '推 '},\n",
       " {'content': '已線型來看支撐水位在66元', 'tm': '04/26 15:48', 'type': '推 '},\n",
       " {'content': '籌募亂亂，主力出爽爽，散戶吃筍', 'tm': '04/26 16:50', 'type': '推 '},\n",
       " {'content': '碼', 'tm': '04/26 16:50', 'type': '→ '},\n",
       " {'content': '表面誠懇哥，私底下就...', 'tm': '04/26 16:51', 'type': '→ '},\n",
       " {'content': '自己想', 'tm': '04/26 16:51', 'type': '→ '},\n",
       " {'content': '他現在還是一直喊這隻.....越喊籌碼越亂整理要越久', 'tm': '04/26 19:44', 'type': '推 '},\n",
       " {'content': '今天利空不跌 是為了明天多跌ㄧ點啊', 'tm': '04/26 20:23', 'type': '推 '},\n",
       " {'content': '不然怎麼騙阿呆進去咧', 'tm': '04/26 20:24', 'type': '→ '},\n",
       " {'content': '51區昨天還在喊，看來今天的早盤那堆都是那邊來加碼', 'tm': '04/27 13:44', 'type': '→ '},\n",
       " {'content': '或攤平的...', 'tm': '04/27 13:44', 'type': '→ '},\n",
       " {'content': '中長線投資 短線不理會他', 'tm': '04/27 13:48', 'type': '→ '},\n",
       " {'content': '打算放到年底了', 'tm': '04/27 13:59', 'type': '→ '},\n",
       " {'content': '樓上大大權證可以放這麼久嗎', 'tm': '04/27 14:12', 'type': '→ '},\n",
       " {'content': '第一季虧0.3元,新產能才開，資產攤提費用高', 'tm': '04/27 14:36', 'type': '推 '},\n",
       " {'content': '他做的鏡頭是紅海市場，良率又是關鍵因素', 'tm': '04/27 14:37', 'type': '推 '},\n",
       " {'content': '這隻純粹喊出來的價位', 'tm': '04/27 14:38', 'type': '推 '},\n",
       " {'content': '只是老闆本厚，唯一大利多', 'tm': '04/27 14:39', 'type': '推 '},\n",
       " {'content': '我是之前套在72現谷是可以放著等，但是51這樣喊法只', 'tm': '04/27 15:33', 'type': '推 '},\n",
       " {'content': '會籌碼更亂', 'tm': '04/27 15:33', 'type': '推 '},\n",
       " {'content': '可以轉其他權證xd', 'tm': '04/27 16:01', 'type': '→ '},\n",
       " {'content': '籌碼凌亂的話...可能就會回測頸線甩轎吧我猜', 'tm': '04/27 18:16', 'type': '→ '}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding push (type, content, tm)\n",
    "push_list = []\n",
    "push_dict = {}\n",
    "pushs = soup2.select('div.push')\n",
    "\n",
    "for push in pushs:\n",
    "    push_dict = {}\n",
    "    push_dict['type'] = push.select_one('.push-tag').text\n",
    "    push_dict['content'] = push.select_one('.push-content').text.replace(':','').strip()\n",
    "    push_dict['tm'] = push.select_one('.push-ipdatetime').text.strip()\n",
    "    push_list.append(push_dict)\n",
    "\n",
    "article_dict['push'] = push_list\n",
    "push_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'ARKUE ()',\n",
       " 'board': 'Stock',\n",
       " 'content': '上來回一下目前成本持有價還沒跌破就更別說停損了今天利空不跌空單今天應該要回補沒補的話後面不一定有機會補除非你跟我一樣長期投資（只是做空）由於是長期投資沒什麼事的話近期我就不再更新了～希望大家多空都能賺錢低手--老實說我蠻欣賞做空能賺到錢的人尤其是修正空單厲害了～',\n",
       " 'ip': '111.82.86.24',\n",
       " 'push': [{'content': '今天51區還在喊盤，這隻完了', 'tm': '04/26 12:54', 'type': '推 '},\n",
       "  {'content': '我不是跟投顧做單的～是自己判斷的', 'tm': '04/26 12:54', 'type': '→ '},\n",
       "  {'content': '融資15%...籌碼很亂，小心為上', 'tm': '04/26 12:55', 'type': '→ '},\n",
       "  {'content': '有賺先跑比較好', 'tm': '04/26 12:55', 'type': '→ '},\n",
       "  {'content': '哈我今天空單也了結了 轉多', 'tm': '04/26 12:56', 'type': '推 '},\n",
       "  {'content': '雷大概念股', 'tm': '04/26 13:03', 'type': '推 '},\n",
       "  {'content': '1500多張的融券 補起來也是會很猛的', 'tm': '04/26 13:05', 'type': '→ '},\n",
       "  {'content': '跟上光學族群 這隻會飆喔', 'tm': '04/26 13:06', 'type': '推 '},\n",
       "  {'content': '另外提醒一下 這檔的特色就是五檔跳動很快 一不小', 'tm': '04/26 13:06', 'type': '→ '},\n",
       "  {'content': '心 就是大漲大跌', 'tm': '04/26 13:06', 'type': '→ '},\n",
       "  {'content': '你是誠懇哥的粉絲齁', 'tm': '04/26 13:14', 'type': '推 '},\n",
       "  {'content': '這支昨天吃筍出一半，好弱', 'tm': '04/26 13:14', 'type': '噓 '},\n",
       "  {'content': '要噴噴了', 'tm': '04/26 13:28', 'type': '推 '},\n",
       "  {'content': '已線型來看支撐水位在66元', 'tm': '04/26 15:48', 'type': '推 '},\n",
       "  {'content': '籌募亂亂，主力出爽爽，散戶吃筍', 'tm': '04/26 16:50', 'type': '推 '},\n",
       "  {'content': '碼', 'tm': '04/26 16:50', 'type': '→ '},\n",
       "  {'content': '表面誠懇哥，私底下就...', 'tm': '04/26 16:51', 'type': '→ '},\n",
       "  {'content': '自己想', 'tm': '04/26 16:51', 'type': '→ '},\n",
       "  {'content': '他現在還是一直喊這隻.....越喊籌碼越亂整理要越久', 'tm': '04/26 19:44', 'type': '推 '},\n",
       "  {'content': '今天利空不跌 是為了明天多跌ㄧ點啊', 'tm': '04/26 20:23', 'type': '推 '},\n",
       "  {'content': '不然怎麼騙阿呆進去咧', 'tm': '04/26 20:24', 'type': '→ '},\n",
       "  {'content': '51區昨天還在喊，看來今天的早盤那堆都是那邊來加碼', 'tm': '04/27 13:44', 'type': '→ '},\n",
       "  {'content': '或攤平的...', 'tm': '04/27 13:44', 'type': '→ '},\n",
       "  {'content': '中長線投資 短線不理會他', 'tm': '04/27 13:48', 'type': '→ '},\n",
       "  {'content': '打算放到年底了', 'tm': '04/27 13:59', 'type': '→ '},\n",
       "  {'content': '樓上大大權證可以放這麼久嗎', 'tm': '04/27 14:12', 'type': '→ '},\n",
       "  {'content': '第一季虧0.3元,新產能才開，資產攤提費用高', 'tm': '04/27 14:36', 'type': '推 '},\n",
       "  {'content': '他做的鏡頭是紅海市場，良率又是關鍵因素', 'tm': '04/27 14:37', 'type': '推 '},\n",
       "  {'content': '這隻純粹喊出來的價位', 'tm': '04/27 14:38', 'type': '推 '},\n",
       "  {'content': '只是老闆本厚，唯一大利多', 'tm': '04/27 14:39', 'type': '推 '},\n",
       "  {'content': '我是之前套在72現谷是可以放著等，但是51這樣喊法只', 'tm': '04/27 15:33', 'type': '推 '},\n",
       "  {'content': '會籌碼更亂', 'tm': '04/27 15:33', 'type': '推 '},\n",
       "  {'content': '可以轉其他權證xd', 'tm': '04/27 16:01', 'type': '→ '},\n",
       "  {'content': '籌碼凌亂的話...可能就會回測頸線甩轎吧我猜', 'tm': '04/27 18:16', 'type': '→ '}],\n",
       " 'time': 'Wed Apr 26 12:49:55 2017',\n",
       " 'title': 'Re: [標的] 3428 光耀科'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding content\n",
    "main_content = soup2.select_one('#main-content')\n",
    "for div in main_content.find_all('div'):\n",
    "    div.decompose()\n",
    "for span in main_content.find_all('span'):\n",
    "    span.decompose()\n",
    "\n",
    "article_dict['content'] = ''.join(main_content.text.split())\n",
    "\n",
    "article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n上來回一下\\n目前成本持有價還沒跌破\\n就更別說停損了\\n今天利空不跌 空單今天應該要回補 沒補的話 後面不一定有機會補 除非你跟我一樣長期\\n投資（只是做空）\\n由於是長期投資 沒什麼事的話 近期我就不再更新了～希望大家多空都能賺錢\\n\\n低\\n手\\n\\n--\\n老實說我蠻欣賞做空能賺到錢的人 尤其是修正空單 厲害了～\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_content = soup2.select_one('#main-content')\n",
    "for div in main_content.find_all('div'):\n",
    "    div.decompose()\n",
    "for span in main_content.find_all('span'):\n",
    "    span.decompose()\n",
    "main_content.text #.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
