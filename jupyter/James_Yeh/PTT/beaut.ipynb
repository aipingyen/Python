{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "/PPT美女版爬蟲程式/\n",
    "/請使用button 輸入數字(int or str)來爬蟲 ：從第一頁開始往後爬的頁數 。 button皆為支援(子)方法/\n",
    "/這個方法的取名來自核子彈按鈕的概念，也如同航海王的'非常召集'，按下(輸入)它你就有種迫不急待的震撼感/\n",
    "\n",
    "purpose：輸入想要爬的頁數\n",
    "\n",
    "輸入：輸入數字,想要爬取頁數\n",
    "輸出：執行mianthread爬蟲程序\n",
    "\n",
    "這裡分兩段，\n",
    "第一段用PPT美女版首頁的連結找出目前的頁數\n",
    "第二段藉由button得到想要爬取的頁數，接著執行爬蟲程式\n",
    "\n",
    "\"\"\"\n",
    "def button(pages):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    HOST = \"https://www.ptt.cc\"\n",
    "    res = requests.get(HOST + \"/bbs/Beauty/index.html\", headers={\"cookie\": \"over18=1;\"})\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    buttons = soup.select('a.btn.wide')\n",
    "    total_page = int(buttons[1]['href'].split('index')[1].split('.html')[0]) + 1\n",
    "\n",
    "    page_to_crawl = int(pages) #決定要爬幾個頁數\n",
    "    for page in range(total_page, total_page - page_to_crawl, -1):\n",
    "        url = 'https://www.ptt.cc/bbs/Beauty/index{}.html'.format(page)\n",
    "        mainthread(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "/主要執行爬蟲的程式碼在這/\n",
    "\n",
    "這裡拆三段\n",
    "1.取出美女版首頁每篇文章的資訊，日期為當天最新的進行\n",
    "2.為頭頁每篇文章做處理，抓取正確資訊並儲存圖片\n",
    "3.json紀錄已更新的資訊\n",
    "\n",
    "抓圖段落下個sleep 可調睡覺時間以防被擋\n",
    "\n",
    "\"\"\"\n",
    "def mainthread(url):\n",
    "    import time\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    articles= {}\n",
    "    page = get_web_page(url)\n",
    "    if page: #判別有無東西\n",
    "        date = time.strftime('%m/%d').lstrip('0') #time抓當天時間-幾月幾號;但bbs時間回傳開頭會有0 所以加lstrip('0')把0幹掉\n",
    "        articles = get_articles(page,date) #蒐集每篇文章的資訊\n",
    "\n",
    "\n",
    "    #開始抓圖\n",
    "    PPT_URL='https://www.ptt.cc'\n",
    "    for article in articles:\n",
    "        page = get_web_page(PPT_URL + article['href']) #抓回來的href沒有www.ptt.cc 自己加\n",
    "        if page:\n",
    "            img_urls = parse(page) #取出每篇的圖片集連結群\n",
    "            save(img_urls,article['title']) #放入每篇圖片集的url跟標題,儲存圖片\n",
    "            article['num_image'] = len(img_urls) #抓出圖片數量，紀錄用\n",
    "            time.sleep(10) #設定睡覺時間 以免被擋掉\n",
    "\n",
    "\n",
    "    import json #存成json檔紀錄\n",
    "    with open('data.json','w',encoding='utf-8') as f:\n",
    "        json.dump(articles,f,indent=2,sort_keys=True,ensure_ascii=False)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "purpos:塞入cookie確認是否滿18;判別requests是否成功\n",
    "\n",
    "輸入:將要解析資訊的網址\n",
    "輸出:回傳request的值，檔案已是text檔!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def get_web_page(url):  \n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    resp = requests.get(url=url,cookies={'over18':'1'}) #塞入cookie over18:1\n",
    "    if resp.status_code != 200:  #用200判別連線是否正常\n",
    "        print('Invalid url:',resp.url)\n",
    "        return None\n",
    "    else:\n",
    "        return resp.text #記住回傳是text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "purpose:抓取'該日'文章內容資訊：標題、網址、案讚數  ;目前這裡是美女版，不是內容頁\n",
    "\n",
    "輸入:美女版已加上cookie的request與想抓取的日期\n",
    "輸出:美女版每個po文的內容資訊\n",
    "\n",
    "1.find_all取得所有內文資訊,\n",
    "2.判斷日期是否正確,\n",
    "3.用find('div','nrec')取讚數,\n",
    "4.用find('a')取出網址跟標題\n",
    "5.取好的資訊放進articles , return出去\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_articles(dom, date):\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(dom, 'html.parser')\n",
    "\n",
    "    articles = []  # 儲存取得的文章資料\n",
    "    divs = soup.find_all('div', 'r-ent') #'r-ent'就是每篇文章的附屬資訊\n",
    "    for d in divs:\n",
    "        #if d.find('div', 'date').string.strip() == date:  # 發文日期正確才繼續 ,這裡原寫法沒strip沒去掉空白，已更正\n",
    "                # 取得推文數\n",
    "        push_count = 0\n",
    "        if d.find('div', 'nrec').string: #nrec→是按讚數\n",
    "            try:  #轉換成功 \n",
    "                push_count = int(d.find('div', 'nrec').string)  # 轉換字串為數字\n",
    "            except ValueError:  # 若轉換失敗，不做任何事，push_count 保持為 0\n",
    "                pass\n",
    "            # 取得文章連結及標題\n",
    "        if d.find('a'):  # 網址跟標題都在a裡面 如果a沒有代表已被版主刪除 用來判斷內文存在不存在\n",
    "            href = d.find('a')['href'] #把a裡面 - href取出來\n",
    "            title = d.find('a').string #把a整串全部抓下來\n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'href': href,\n",
    "                'push_count': push_count\n",
    "            })\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "purpose:取出圖片網址群\n",
    "\n",
    "輸入:當篇html碼\n",
    "輸出:當篇的圖片網址\n",
    "\n",
    "BeautifulSoup回傳的是text,\n",
    "用find → id='main-contect'找出圖片的區塊,再用find_all找到所有 a的內容\n",
    "\n",
    "for loop:從link['href'], href的class,用寫好的正規表示法抽出想要的圖片網址\n",
    "img_urls:要回傳圖片網址集,裡面都是整理好的網址\n",
    "\n",
    "\"\"\"\n",
    "def parse(dom): #過濾出乾淨的圖片網址LIST\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    soup = BeautifulSoup(dom,'html.parser') #用html.parser引擎\n",
    "    links = soup.find(id='main-content').find_all('a') #圖片的標籤\n",
    "    img_urls = []\n",
    "    for link in links:\n",
    "        if re.match(r'^https?://(i.)?(m.)?imgur.com', link['href']): #爭歸表示法\n",
    "            img_urls.append(link['href']) #每抽出一次新增到img_urls裡面\n",
    "    return img_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "purpose:把檔案存進建好的路徑\n",
    "\n",
    "輸入:存好的圖片群,圖片的標題群\n",
    "輸出:資料夾,與存好在資歷夾的jpg檔案\n",
    "\n",
    "dname:把標題名稱切乾淨\n",
    "for loop:抓到的檔案url名稱不乾淨 要統一種格式才能下載\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def save(img_urls,title):  \n",
    "    import os\n",
    "    import urllib\n",
    "    if img_urls:\n",
    "        try:\n",
    "            if not title.strip().find('Re:') != -1:#去除會因為'Re:'命名而無法建資料夾的狀況\n",
    "                title.strip().replace('Re:','')\n",
    "            dname = title.strip() #標題旁邊去空白\n",
    "            os.makedirs(dname) #創建資料夾\n",
    "            for img_url in img_urls: # 下載的時候必須是i.imgur.com  以下處理不是的\n",
    "                if img_url.split('//')[1].startswith('m.'):  #如果開頭是 m.imgur.com  把m換成i.\n",
    "                    img_url = img_url.replace('//m.','//i.')\n",
    "                if not img_url.split('//')[1].startswith('i.'): #如果開頭不是i. 就加上去i.\n",
    "                    img_url = img_url.split('//')[0]+'//i.'+img_url.split('//')[1]\n",
    "                if not img_url.endswith('.jpg'):  #如果尾巴沒有.jpg 加上個jpg檔名\n",
    "                    img_url += '.jpg'\n",
    "                fname = img_url.split('/')[-1] #切出最後段當作檔案名稱\n",
    "                urllib.request.urlretrieve(img_url, os.path.join(dname,fname)) #放的資料路徑,os.path.join(資料夾,檔名)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
