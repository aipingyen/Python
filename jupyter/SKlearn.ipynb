{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col1  col2\n",
      "0     1     3\n",
      "1     2     4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "my_2d_array = np.array([[1, 3],\n",
    "                        [2, 4]\n",
    "                       ])\n",
    "\n",
    "my_df = pd.DataFrame(my_2d_array, columns = [\"col1\", \"col2\"])\n",
    "print(my_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       groups  ironmen\n",
      "0  Modern Web       46\n",
      "1      DevOps        8\n",
      "2       Cloud       12\n",
      "3    Big Data       12\n",
      "4    Security        6\n",
      "5       自我挑戰組       58\n",
      "('Modern Web', 'DevOps', 'Cloud', 'Big Data', 'Security', '自我挑戰組')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # 引用套件並縮寫為 pd\n",
    "\n",
    "groups = [\"Modern Web\", \"DevOps\", \"Cloud\", \"Big Data\", \"Security\"]\n",
    "groups.insert(5, \"自我挑戰組\")\n",
    "\n",
    "ironmen = [46, 8, 12, 12, 6, 58]\n",
    "\n",
    "ironmen_dict = {\"groups\": groups,          # key : val(list)\n",
    "                \"ironmen\": ironmen\n",
    "                }\n",
    "\n",
    "ironman_groups_tuple = tuple(groups)\n",
    "\n",
    "ironmen_df = pd.DataFrame(ironmen_dict)   # dict => df\n",
    "\n",
    "print(ironmen_df) # 看看資料框的外觀\n",
    "print(ironman_groups_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       groups  ironmen\n",
      "0  Modern Web       46\n",
      "2       Cloud       12\n",
      "3    Big Data       12\n",
      "5       自我挑戰組       58\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "groups = [\"Modern Web\", \"DevOps\", \"Cloud\", \"Big Data\", \"Security\", \"自我挑戰組\"]\n",
    "ironmen = [46, 8, 12, 12, 6, 58]\n",
    "\n",
    "ironmen_dict = {\"groups\": groups,\n",
    "                \"ironmen\": ironmen\n",
    "                }\n",
    "\n",
    "ironmen_df = pd.DataFrame(ironmen_dict)\n",
    "\n",
    "print(ironmen_df[ironmen_df.loc[:,\"ironmen\"] > 10]) # 選出鐵人數超過 10 的 data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inherit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OnlyGroup:\n",
    "    '''這是一個叫做 OnlyGroup 的類別''' # Doc string\n",
    "    def __init__(self, group):\n",
    "        self.group = group\n",
    "\n",
    "# Ironmen 類別繼承 OnlyGroup 類別\n",
    "class Ironmen(OnlyGroup):\n",
    "    '''這是一個叫做 Ironmen 的類別''' # Doc string\n",
    "    def __init__(self, group, participants):\n",
    "        super().__init__(group)\n",
    "        self.participants = participants\n",
    "\n",
    "# 根據 Ironmen 類別建立一個物件 modern_web\n",
    "modern_web = Ironmen(\"Modern Web\", 54)\n",
    "\n",
    "# 印出 modern_web 的兩個屬性\n",
    "print(modern_web.group)\n",
    "print(modern_web.participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Ironmen:\n",
    "    '''這是一個叫做 Ironmen 的類別''' # Doc string\n",
    "    def __init__(self, group, participants):\n",
    "        self.group = group\n",
    "        self.participants = participants\n",
    "    \n",
    "    def print_info(self):\n",
    "        print(self.group, \"組有\", self.participants, \"位鐵人參賽！\")\n",
    "\n",
    "# Articles 類別繼承 Ironmen 類別\n",
    "class Articles(Ironmen):\n",
    "    '''\n",
    "    這是一個叫做 Articles 的類別。\n",
    "    Articles 繼承 Ironmen 類別，她新增了一個 print_articles() 方法\n",
    "    '''\n",
    "    def print_articles(self):\n",
    "        print(self.group, \"組預計會有\", self.participants * 30, \"篇文章！\")\n",
    "    \n",
    "    # 改寫 print_info() 方法\n",
    "    def print_info(self):\n",
    "        print(self.group, \"組有\", self.participants, \"位鐵人參賽！p.s.我被改寫了！\")\n",
    "\n",
    "# 根據 Articles 類別建立一個物件 modern_web\n",
    "modern_web = Articles(\"Modern Web\", 54)\n",
    "\n",
    "# 檢查 modern_web 的 print_info() 方法是否被改寫\n",
    "modern_web.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  監督式(supervised)  迴歸與分類 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迴歸（Regression）  預測連續型目標變數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線性迴歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "(150, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                  5.1               3.5                1.4               0.2   \n",
       "1                  4.9               3.0                1.4               0.2   \n",
       "2                  4.7               3.2                1.3               0.2   \n",
       "3                  4.6               3.1                1.5               0.2   \n",
       "4                  5.0               3.6                1.4               0.2   \n",
       "5                  5.4               3.9                1.7               0.4   \n",
       "6                  4.6               3.4                1.4               0.3   \n",
       "7                  5.0               3.4                1.5               0.2   \n",
       "8                  4.4               2.9                1.4               0.2   \n",
       "9                  4.9               3.1                1.5               0.1   \n",
       "10                 5.4               3.7                1.5               0.2   \n",
       "11                 4.8               3.4                1.6               0.2   \n",
       "12                 4.8               3.0                1.4               0.1   \n",
       "13                 4.3               3.0                1.1               0.1   \n",
       "14                 5.8               4.0                1.2               0.2   \n",
       "15                 5.7               4.4                1.5               0.4   \n",
       "16                 5.4               3.9                1.3               0.4   \n",
       "17                 5.1               3.5                1.4               0.3   \n",
       "18                 5.7               3.8                1.7               0.3   \n",
       "19                 5.1               3.8                1.5               0.3   \n",
       "20                 5.4               3.4                1.7               0.2   \n",
       "21                 5.1               3.7                1.5               0.4   \n",
       "22                 4.6               3.6                1.0               0.2   \n",
       "23                 5.1               3.3                1.7               0.5   \n",
       "24                 4.8               3.4                1.9               0.2   \n",
       "25                 5.0               3.0                1.6               0.2   \n",
       "26                 5.0               3.4                1.6               0.4   \n",
       "27                 5.2               3.5                1.5               0.2   \n",
       "28                 5.2               3.4                1.4               0.2   \n",
       "29                 4.7               3.2                1.6               0.2   \n",
       "..                 ...               ...                ...               ...   \n",
       "120                6.9               3.2                5.7               2.3   \n",
       "121                5.6               2.8                4.9               2.0   \n",
       "122                7.7               2.8                6.7               2.0   \n",
       "123                6.3               2.7                4.9               1.8   \n",
       "124                6.7               3.3                5.7               2.1   \n",
       "125                7.2               3.2                6.0               1.8   \n",
       "126                6.2               2.8                4.8               1.8   \n",
       "127                6.1               3.0                4.9               1.8   \n",
       "128                6.4               2.8                5.6               2.1   \n",
       "129                7.2               3.0                5.8               1.6   \n",
       "130                7.4               2.8                6.1               1.9   \n",
       "131                7.9               3.8                6.4               2.0   \n",
       "132                6.4               2.8                5.6               2.2   \n",
       "133                6.3               2.8                5.1               1.5   \n",
       "134                6.1               2.6                5.6               1.4   \n",
       "135                7.7               3.0                6.1               2.3   \n",
       "136                6.3               3.4                5.6               2.4   \n",
       "137                6.4               3.1                5.5               1.8   \n",
       "138                6.0               3.0                4.8               1.8   \n",
       "139                6.9               3.1                5.4               2.1   \n",
       "140                6.7               3.1                5.6               2.4   \n",
       "141                6.9               3.1                5.1               2.3   \n",
       "142                5.8               2.7                5.1               1.9   \n",
       "143                6.8               3.2                5.9               2.3   \n",
       "144                6.7               3.3                5.7               2.5   \n",
       "145                6.7               3.0                5.2               2.3   \n",
       "146                6.3               2.5                5.0               1.9   \n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     species  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "5          0  \n",
       "6          0  \n",
       "7          0  \n",
       "8          0  \n",
       "9          0  \n",
       "10         0  \n",
       "11         0  \n",
       "12         0  \n",
       "13         0  \n",
       "14         0  \n",
       "15         0  \n",
       "16         0  \n",
       "17         0  \n",
       "18         0  \n",
       "19         0  \n",
       "20         0  \n",
       "21         0  \n",
       "22         0  \n",
       "23         0  \n",
       "24         0  \n",
       "25         0  \n",
       "26         0  \n",
       "27         0  \n",
       "28         0  \n",
       "29         0  \n",
       "..       ...  \n",
       "120        2  \n",
       "121        2  \n",
       "122        2  \n",
       "123        2  \n",
       "124        2  \n",
       "125        2  \n",
       "126        2  \n",
       "127        2  \n",
       "128        2  \n",
       "129        2  \n",
       "130        2  \n",
       "131        2  \n",
       "132        2  \n",
       "133        2  \n",
       "134        2  \n",
       "135        2  \n",
       "136        2  \n",
       "137        2  \n",
       "138        2  \n",
       "139        2  \n",
       "140        2  \n",
       "141        2  \n",
       "142        2  \n",
       "143        2  \n",
       "144        2  \n",
       "145        2  \n",
       "146        2  \n",
       "147        2  \n",
       "148        2  \n",
       "149        2  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use sklearn 的 datasets 物件的 load_iris()\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "print(type(iris.data)) # 資料是儲存為 ndarray\n",
    "print(iris.feature_names) # 變數名稱可以利用 feature_names 屬性取得\n",
    "print(iris.data.shape)   #150*4\n",
    "\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names) # 轉換為 data frame\n",
    "iris_df.ix[:, \"species\"] = iris.target # 將品種加入 data frame   Iris setosa, Iris virginica and Iris versicolor\n",
    "iris_df.head() # 觀察前五個觀測值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.73788546]]\n",
      "[-36.36123348]\n"
     ]
    }
   ],
   "source": [
    "#Use sklearn.linear_model 的 LinearRegression()\n",
    "#可與keras的LinearRegression相比\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "temperatures = np.array([29, 28, 34, 31, 25, 29, 32, 31, 24, 33, 25, 31, 26, 30])\n",
    "iced_tea_sales = np.array([77, 62, 93, 84, 59, 64, 80, 75, 58, 91, 51, 73, 65, 84])\n",
    "\n",
    "# 轉換維度  1*14  =>  14*1\n",
    "temperatures = np.reshape(temperatures, (len(temperatures), 1))\n",
    "iced_tea_sales = np.reshape(iced_tea_sales, (len(iced_tea_sales), 1))\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(temperatures, iced_tea_sales, 1)\n",
    "                #lm training data \n",
    "# 印出係數\n",
    "print(lm.coef_)\n",
    "\n",
    "# 印出截距\n",
    "print(lm.intercept_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 75.7753304]]\n"
     ]
    }
   ],
   "source": [
    "# 新的氣溫\n",
    "to_be_predicted = np.array([30])\n",
    "predicted_sales = lm.predict(np.reshape(to_be_predicted, (len(to_be_predicted), 1)))\n",
    "                      #use LinearRegression model to predict \n",
    "# 預測的冰紅茶銷量\n",
    "print(predicted_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGsFJREFUeJzt3XtwXOWZ5/HvI98FCZexsLlE3QY8EzwuxyZaGDYMzOAk\nNTipMbWpYpPRppwLVhgyuwNssgOjYpYMpaqEgc1kakKGLjxgV3XCZeMEV7jEXhNSSYgNMsbrGJIA\nxi1iyzYhhmQj2zLSs3+clq2WjqSW1efSp3+fKpfUb7d9npOgn18/57zvMXdHRETqX1PSBYiISG0o\n0EVEMkKBLiKSEQp0EZGMUKCLiGSEAl1EJCMU6CIiGaFAFxHJCAW6iEhGTI/zYHPnzvV8Ph/nIUVE\n6t62bdt+7e4tE30u1kDP5/N0d3fHeUgRkbpnZqVqPqeWi4hIRijQRUQyQoEuIpIRCnQRkYxQoIuI\nZIQCXUQkIsVikXw+T1NTE/l8nmKxGOnxYr1tUUSkURSLRTo6Oujr6wOgVCrR0dEBQHt7eyTH1Axd\nRCQCnZ2dx8N8SF9fH52dnZEdU4EuIhKBnp6eSY3XggJdRCQCra2tkxqvBQW6iEgEurq6aG5urhhr\nbm6mq6srsmMq0EVEItDe3k6hUCCXy2Fm5HI5CoVCZBdEAczdI/vDR2pra3NtziUiMjlmts3d2yb6\nnGboIiIZoUAXEckIBbqIZF7cKzaTopWiIpJpSazYTIpm6CKSaUms2EyKAl1EMi2JFZtD1q8HM2hp\ngYMHIz9cdYFuZn9rZj8zs11mdmN57Ewz22RmL5e/nhFtqSIik5fEis3du4Mg/9jHgte//jXEcYf4\nhIFuZouB1cAlwPuAj5rZhcAtwGZ3XwhsLr8WEUmVOFds9vfD0qVwwQWV47t2wbx5NT/cKNXM0C8C\ntrp7n7u/A/wQ+E/ASmBt+TNrgWuiKVFE5OTFtWLz9tth1izYsePE2AMPBDPzRYtqeqgxTbhS1Mwu\nAh4FLgMOE8zGu4FPuvvp5c8YcGjo9Yjf3wF0ALS2tr6/VCrV9ARERJL0gx/AVVdVjl17LTz4YNB2\nqYVqV4pOeNuiu79kZl8BNgK/B14ABkZ8xs0s9G8Gdy8ABQiW/ldRu4hI6h08OLqNMmMG7N8PZ56Z\nTE1VXRR19zXu/n53vwI4BPwSOGBmZwOUv8ZwDVdEJFmDg7Bixegwf+aZoIeeVJhD9Xe5nFX+2krQ\nP/8msAFYVf7IKoK2jIhIZn3jGzBtGjzxxImxO+8M+uSXXZZcXUOqXSn6bTP7A+AY8Hl3f8vMvgw8\nbGafBUrAtVEVKSKSpBdegGXLKscuvzzon09P0Xr7qkpx9z8NGXsTWF7zikREUuK3v4Xzz4c336wc\nf/11OO+8ZGoaj1aKioiM4A7XXQennVYZ5o89FryXxjAHBbqISIVHHoGmJliz5sTYzTcHQb5iRXJ1\nVSNF3R8RkeS8+ipceGHl2AUXwM6dMGdOMjVNlmboItLQjh6FJUtGh/mLL8Irr9RPmIMCXUQa2G23\nwezZwSx8yLp1QXvloouSq+tkqeUiIg3nqadg+Yh79D7xCSgWa7dcPwkKdBFpGAcOwPz5lWOzZ8O+\nfXBGBjYAV8tFRDLv2LFg5j0yzLdsgcOHsxHmoEAXkYxbtAhmzqwcu+uuoE9+6aXJ1BQVtVxEJJPu\nuw9Wrx49fuxYupbr11JGT0tEGtXrr0PY0+Weew7aJtxRvL6p5SIimeAe9MlHhvlNNwXvZT3MQTN0\nEcmAj30M1q8fPR7Hg5nTRDN0EalbTz4ZzMpHhvlvftN4YQ4KdBGpQ2+/HQT51VdXjm/YEAR5Vm5D\nnCy1XESkroSt5PzIR+B734u/lrTRDF1E6sKtt4aH+eCgwnyIZugikmrbt8PFF48e37MHcrnYy0k1\nzdBFJJX6+4MZ+cgwv+eeoE+uMB9NM3QRSZ2FC4O9yIfL5+G11xIpp25ohi6SkGKxSD6fp6mpiXw+\nT7FYzPRxq1EoBLPykWF+9KjCvBqaoYskoFgs0tHRQV9fHwClUomOjg4A2tvbM3fcifT0hLdQtm0L\n759LOPMY775va2vz7u7u2I4nklb5fJ5SqTRqPJfLsWfPnswddyzuwQOZR/riF+HOO2MvJ7XMbJu7\nT7h5gWboIgno6emZ1Hi9HzfMypXBQqCRGnGFZ62ohy6SgNaw7QDHGa/34w73+ONBn3xkmB86pDCf\nKgW6SAK6urpobm6uGGtubqarqyuTx4Xg8W9mwarO4R57LAjy00+PvITMU6CLJKC9vZ1CoUAul8PM\nyOVyFAqFyC9MJnXcsMe/rVwZBPmKFZEeuqHooqiIRGbpUtixY/T44GD4Mn4JV+1FUc3QRaTmHnss\nCOyRYb5z54kHUUjt6S4XEamZw4dhRIsegM99Dv7t3+Kvp9Eo0EWkJsaadevOlfio5SIiU/KZz4SH\n+eHDCvO4KdBF5KTs2BEE+f33V44/+WQQ5LNnJ1NXI1PLRUQmZXAQpk0bPX7JJbB1a/z1yAkKdBGp\nmvrk6aaWi4hM6O67w8P84EGFeZpohi4iY+rthXPOGT3+9a/DDTfEX4+MT4EuIqHUXqk/VbVczOwm\nM9tlZj8zs2+Z2WwzO9PMNpnZy+WvZ0RdrIhE74//ODzMBwenFuZpflJSVkwY6GZ2LvDfgDZ3XwxM\nAz4O3AJsdveFwObyaxGpU48+GgT5iy9Wjr/44tSX6w89KalUKuHux5+UpFCvrWovik4H5pjZdKAZ\n2AesBNaW318LXFP78kQkan19QVhfM+In+G/+Jgjyiy6a+jE6OzuPP/buxHH76OzsnPofLsdN2EN3\n971mdhfQAxwGNrr7RjOb5+695Y/tB+aF/X4z6wA6IN5N9EVkYnH1ydP0pKQsq6blcgbBbHwBcA5w\nipn9l+Gf8WAP3tD/BNy94O5t7t7W0tJSg5JFZKo+/OHwMD9yJJqLnml4UlIjqKbl8kHgNXd/w92P\nAeuB/wgcMLOzAcpfD0ZXpojUwlNPBUG+aVPl+KZNQZDPmhXNcZN8UlIjqSbQe4A/MbNmMzNgOfAS\nsAFYVf7MKuDRaEoUkakaGAiCfPnyyvEPfCAI8g9+MNrjJ/WkpEZT1ROLzOxLwH8G3gG2A9cBpwIP\nA61ACbjW3X8z3p+jJxaJxE/3k9e/mj6xyN3/p7u/190Xu/sn3f2ou7/p7svdfaG7f3CiMBeReH36\n0+FhvnevwjyrtJeLSMa88koQ5A88UDn+d38XBHnYUn7JBi39F8kQtVcamwJdJAPGCvLBQT2QuZGo\n5SJSx/7lX8IDu7t76sv1pf5ohi5Sh95+G04/ffT4VVfB5s3x1yPpoEAXqTPqk8tY1HKRhlZPW7ou\nWBAe5ocPK8wloECXhlUvW7pu3BgE+Z49leMPPRQE+ezZiZQlKVTVStFa0UpRSZN8Pk+pVBo1nsvl\n2DMyPRMwMADTQ5qiTU3Be9I4arpSVCSLUrGla28vXHkl7N9fMWwWHubuCnMZmwJdGlYqtnS94w74\n8Y+Dr8Bf/VV4n3z/fvXJZWIKdGlYiW/p2tsL998Pg4MMrrmf+bafb32r8iO33RYE+bzQx8eIVFKg\nS8NKfEvXO+4IlnIC/UcHuI07Kt52h3/8x3hKkWzQRVGRJPT2cvic85nDkeNDfczhfHbTOzhfKzyl\ngi6KiqTU7bfDPefcgTFYMT57xgD7b7hDYS4nTYEuEpM33wwueN77pV4+zf3Mpr/i/aZj/UFPfcQd\nL7VWT4upZHIU6CIxMIO5c4Pvb2P07Py4gYHjd7xEoV4WU8nJUQ9dJEIj2yfz6WU3lb3zUebMgd27\nYf78mteT9sVUEk49dJEEffvb4feTP33VHcyZOcbsfEiEs/RULKaSyGi3RZEaOnYMZs4Mf88dWPZT\n6O8P/8CQ/n545pma1wbBoqmwGXqsi6kkMpqhi9SIWXiYuw9b5bl9+4mB8X5t3x5JjYkvppJIKdBF\npujP/iy8vdLbm77l+okvppJI6aKoyEnasQOWLh09fvPNcPfd8dcj2VXtRVH10EVOgp4aJGmklovI\nJJiFh/ngYP2EuRYWZZcCXaQKt94aHuTd3UGQ18tyfS0syjb10EXGcfBg+Na1l14KW7bEX89UaWFR\nfVIPXWSKstgn18KibFPLRWSEsfrkR4/Wd5hDSp7SJJFRoIuUPfhgeJA/+GAQ5GOtAK0nWliUbWq5\nSMPr74dZs8Lfq/cZ+UhDC4g6Ozvp6emhtbWVrq4uLSzKCF0UlYaWxT65ZI92WxQZx4UXhof5wYMK\nc6lfCnRpKFu3BkH+6quV49dfHwR5S0sydYnUgnro0jDUXpGsU6BL5inIpVGo5SKZtXp1eJg/+6zC\nXLJJM3TJnH374NxzR48vWgS7dsVfj0hcJgx0M/sj4KFhQ+cD/wCsK4/ngT3Ate5+qPYlilRP7RVp\nZBO2XNz9F+6+1N2XAu8H+oDvALcAm919IbC5/FokEWMt1+/vV5hL45hsD3058Kq7l4CVwNry+Frg\nmloWJlKNQiE8yNetC4J8xoz4axJJymR76B8HvlX+fp6795a/3w+EbDIKZtYBdIA2AJLaOXIE5swJ\nf08zcmlUVc/QzWwm8JfAIyPf82D/gNAfI3cvuHubu7e1aNWG1IBZeJi7K8ylsU2m5XI18Ly7Hyi/\nPmBmZwOUvx6sdXEiw517bnh75cABBbkITC7QP8GJdgvABmBV+ftVwKO1KkpkuJ/8JAjyffsqx2+8\nMQjys85Kpi6RtKmqh25mpwAfAj43bPjLwMNm9lmgBFxb+/KkkblD0xhTDs3IRUarKtDd/ffAH4wY\ne5PgrheRmtP95CKTp6X/kiqrVoWH+fPPK8xFJqKl/5IKBw7A/Pmjx5cuhe3b469HpB4p0CVxaq+I\n1IZaLpKY6dPDw/zYMYW5yMlQoEvsHn44CPKBgcrx738/CPLp+nejyEnRj47E5uhRmD179HguB3v2\nxF6OSOYo0CUW6pOLRE8tF4nU8uXhYf7WWwpzkVpToEsknn8+CPKnnqoc/+d/DoL8tNOSqUsky9Ry\nkZrScn2R5CjQpWbUJxdJllouMmW33hoe5i+/rDAXiZNm6HLS9u0L9igfadUqeOCB2MsRaXgKdDkp\naq+IpI9aLjIpZuFh/s47CnORpCnQU6hYLJLP52lqaiKfz1MsFpMuiW9+MzzIN28OgnzatPhrEpFK\narmkTLFYpKOjg76+PgBKpRIdHR0AtLe3x17PkSPhD2ReuBB++cvYyxGRcZjH+O/ktrY27+7uju14\n9Sifz1MqlUaN53I59sS84Yn65CLpYGbb3L1tos+p5ZIyPT09kxqPwhVXhIf5228rzEXSTIGeMq2t\nrZMar6Vnnw2C/Ec/qhz/+teDIH/3uyMvQUSmQIGeMl1dXTQ3N1eMNTc309XVFdkx3YMgv/TS8Pdu\nuCGyQycujRegRU6WAj1l2tvbKRQK5HI5zIxcLkehUIjsgqhZ+N4r7tlvrwxdgC6VSrj78QvQCnWp\nV7oo2qC+8AW4++7R47t3w4IF8deThDRdgBYZT7UXRXXbYoPZuxfOO2/0+OrVUCjEX0+S0nABWqSW\nFOgNRLchVmptbQ2docdxAVokCuqhN4CxlusPDDRumEMyF6BFoqRAz7B168KD/Omnx38QRaOI+wK0\nSNR0UTSD+vrglFNGjy9eDDt3xl+PiEyNLoo2KPXJRRpXg/+jOzsuvTQ8zH/3u/oIcy3wEZk6BXqd\n++lPgyB/9tnK8UIhCPJTT02mrsnQAh+R2lAPvU6Nd1GzHmbkw2mBj8j41EPPsKz1ybXAR6Q21HKp\nIzfeGB7me/bUb5hDsjtMimSJAr0O9PQEQf61r1WOf/7zQZDncsnUVSta4CNSG2q5pFzW2ithhhby\ndHZ20tPTQ2trK11dXVrgIzJJuiiaUtOmweDg6PGBAa3wFGk0egRdnXr88WBWPjLMf/xjLdcXkfFV\n1XIxs9OB+4DFgAOfAX4BPATkgT3Ate5+KJIqG8CRIzBnzujxiy+Gbdvir0dE6k+1872vAU+6+3uB\n9wEvAbcAm919IbC5/FpOgll4mLsrzEWkehMGupmdBlwBrAFw9353fwtYCawtf2wtcE1URWbVddeF\nX/Q8fDhbFz1FJB7VzNAXAG8A95vZdjO7z8xOAea5e2/5M/uBeVEVmTU7dwZBvmZN5fjjjwdBPnt2\nMnWJSH2rJtCnAxcD33D3ZcDvGdFe8eBWmdA5pZl1mFm3mXW/8cYbU623rrkHQb5kSeV4W1vw3tVX\nJ1OXiGRDNYH+K+BX7r61/Pp/EwT8ATM7G6D89WDYb3b3gru3uXtbS0tLLWqOTS13ADQLv0PFHZ57\nbgpFioiUTRjo7r4feN3M/qg8tBx4EdgArCqPrQIejaTChNRqB8CvfjW8T37woPrkIlJbVS0sMrOl\nBLctzgR2A58m+MvgYaAVKBHctvib8f6celpYNNUdAHt74ZxzRo//678GS/ZFRKpV090W3f0FIOwP\nWz7ZwurFVHYAbITl+iKSPlp3OIaT2QFwyZLwMB8cVJiLSPRSH+hJPZpsMjsAbtgQBPnIBzDv2nXi\nzhYRkailOtCTfDRZe3s7hUKBXC6HmZHL5SgUChU7APb1BWG9cmXl773hhiDIFy2KvEwRkeNSvdti\nmh9Npj65iMQlE7stpvHRZJ/6VHiYHzmiMBeRZKU60NP0aLIXXgiCfO3ayvGNG4MgnzUr9pJERCqk\nOtDT8GiywcEgyJctqxy/7LIgyD/0odhKEREZV6oDvZoLk1EqFIInB43kDs88E0sJIiJVS/VF0aT8\n4hfw3veOHn/jDZg7N/56RKSxZeKiaNwOH4YLLxwd5j/8YTArV5iLSJop0Mu+8AVoboZXXz0x9sgj\nQZBfcUVydYmIVKuqvVyy7IknYMWKyrHVq+Hee7XCU0TqS8MG+t69cN55lWMtLcEM/V3vSqYmEZGp\naLiWyzvvBC2UkWG+fXuwR7nCXETqVUMF+l13wYwZ8KMfnRi7556gT750aXJ1iYjUQkO0XLZsCRYC\nDfeRjwS7JIY9Fk5EpB5lOtAPHYL586G/v3L8wAE466xkahIRiUom56fu8PGPw5lnVob5008H7ynM\nRSSLMhfo69YFbZSHHjoxdvvtQZBfeWViZYmIRC4zLZeXXhr9QIlly4L++cyZydQkIhKnug/0vj5Y\nvBhee61yfPduWLAgmZpERJJQ1y2Xm2+GU06pDPP164P2isJcRBpNXc7QH3sMPvrRyrHrrw/uKddy\nfRFpVHUV6K+/DiMfVjRvHrzyCpx6ajI1iYikRV20XAYG4PLLR4f5jh2wf7/CXEQE6iTQ77sPfvKT\nE6/vvTfoky9ZklxNIiJpUxctl2XLgs20li2D735Xy/VFRMLURaBfcknQPxcRkbFprisikhEKdBGR\njFCgi4hkhAJdRCQjFOgiIhmhQBcRyQgF+jiKxSL5fJ6mpiby+TzFYjHpkkRExlQX96EnoVgs0tHR\nQV9fHwClUomOjg4A2tvbkyxNRCSUZuhj6OzsPB7mQ/r6+ujs7EyoIhGR8SnQx9DT0zOpcRGRpFUV\n6Ga2x8x2mtkLZtZdHjvTzDaZ2cvlr2dEW2q8Wkdu7TjBuIhI0iYzQ/9zd1/q7m3l17cAm919IbC5\n/Dozurq6aG5urhhrbm6mq6sroYpERMY3lZbLSmBt+fu1wDVTLyc92tvbKRQK5HI5zIxcLkehUNAF\nURFJLXP3iT9k9hrwNjAA3OvuBTN7y91PL79vwKGh12Npa2vz7u7uGpQtItI4zGzbsO7ImKq9bfFy\nd99rZmcBm8zs58PfdHc3s9C/GcysA+gA9Z9FRKJUVcvF3feWvx4EvgNcAhwws7MByl8PjvF7C+7e\n5u5tLS0ttalaRERGmTDQzewUM3vX0PfAh4GfARuAVeWPrQIejapIERGZWDUtl3nAd4I2OdOBb7r7\nk2b2HPCwmX0WKAHXRlemiIhMZMJAd/fdwPtCxt8ElkdRlIiITF5Vd7nU7GBmbxDM5k/GXODXNSyn\nHuicG0OjnXOjnS9M/Zxz7j7hRchYA30qzKy7mtt2skTn3Bga7Zwb7XwhvnPWXi4iIhmhQBcRyYh6\nCvRC0gUkQOfcGBrtnBvtfCGmc66bHrqIiIyvnmboIiIyjlQGupm9x8x+YGYvmtkuM/vbEe//dzNz\nM5ubVI21Nt45m9l/NbOfl8fvTLLOWhnrfM1sqZltGdp738wuSbrWWjGz2Wb2rJntKJ/zl8rjmX22\nwDjn/E/l/6b/r5l9x8zG3divnox1zsPejy6/3D11v4CzgYvL378L+CWwqPz6PcD3Ce5nn5t0rVGf\nM/DnwP8BZpXfOyvpWiM+343A1eXxFcDTSddaw3M24NTy9zOArcCfAHcCt5THbwG+knStMZzzh4Hp\n5fGvNMI5l19Hml+pnKG7e6+7P1/+/nfAS8C55be/CvwPIFPN/3HO+a+BL7v70fJ7oZug1ZtxzteB\nd5c/dhqwL5kKa88D/6/8ckb5l5PhZwuMdc7uvtHd3ymPbwHOS6TACIzz/zNEnF+pDPThzCwPLAO2\nmtlKYK+770i0qIgNP2fgD4E/NbOtZvZDM/sPSdYWhRHneyPwT2b2OnAXcGtyldWemU0zsxcIdifd\n5O5bgXnu3lv+yH6C/ZMyY4xzHu4zwBPxVxadsHOOI79SHehmdirwbYIf8neAvwf+IdGiIjb8nN39\ntwT77ZxJ8M/ULxJsiGYJllhTIef718BN7v4e4CZgTZL11Zq7D7j7UoIZ6SVmtnjE+072/vU55jmb\nWSfBz3YxqfqiEHLOS4ghv1Ib6GY2g+AHveju64ELgAXADjPbQ/A/1PNmNj+5Kmsr5JwBfgWsL/8z\n7llgkGBfiLo3xvmuAoa+f4Rg7/3Mcfe3gB8Af0GVzxaodyPOGTP7FPBRoL38F1nmDDvnlcSQX6kM\n9PIMdA3wkrv/LwB33+nuZ7l73t3zBEF3sbvvT7DUmgk757LvElwYxcz+EJhJBjY2Gud89wFXlr+/\nCng57tqiYmYtQ3dzmNkc4EPAz8nwswXGOmcz+wuCXvJfuntfkjXW2hjnvD2O/Kr2EXRx+wDwSWBn\nuQ8F8Pfu/niCNUUt9JyBfwf+3cx+BvQDqzIymxnrfFcDXzOz6cARyo8vzIizgbVmNo1gMvWwu3/P\nzH5Kdp8tMNY5vwLMInikJcAWd78+wTprKfSc4ziwVoqKiGREKlsuIiIyeQp0EZGMUKCLiGSEAl1E\nJCMU6CIiGaFAFxHJCAW6iEhGKNBFRDLi/wMSZb/hs9XsUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27d701addd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "temperatures = np.array([29, 28, 34, 31, 25, 29, 32, 31, 24, 33, 25, 31, 26, 30])\n",
    "iced_tea_sales = np.array([77, 62, 93, 84, 59, 64, 80, 75, 58, 91, 51, 73, 65, 84])\n",
    "\n",
    "# 轉換維度  1*14  =>  14*1\n",
    "temperatures = np.reshape(temperatures, (len(temperatures), 1))\n",
    "iced_tea_sales = np.reshape(iced_tea_sales, (len(iced_tea_sales), 1))\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(temperatures, iced_tea_sales)\n",
    "\n",
    "# 新的氣溫\n",
    "to_be_predicted = np.array([30])\n",
    "predicted_sales = lm.predict(np.reshape(to_be_predicted, (len(to_be_predicted), 1)))\n",
    "\n",
    "# 視覺化\n",
    "plt.scatter(temperatures, iced_tea_sales, color='black')\n",
    "plt.plot(temperatures, lm.predict(np.reshape(temperatures, (len(temperatures), 1))), color='blue', linewidth=2)\n",
    "plt.plot(to_be_predicted, predicted_sales, color = 'red', marker = '^', markersize = 10)\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.9348646948\n",
      "0.822509288117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "temperatures = np.array([29, 28, 34, 31, 25, 29, 32, 31, 24, 33, 25, 31, 26, 30])\n",
    "iced_tea_sales = np.array([77, 62, 93, 84, 59, 64, 80, 75, 58, 91, 51, 73, 65, 84])\n",
    "\n",
    "# 轉換維度\n",
    "temperatures = np.reshape(temperatures, (len(temperatures), 1))\n",
    "iced_tea_sales = np.reshape(iced_tea_sales, (len(iced_tea_sales), 1))\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(temperatures, iced_tea_sales)\n",
    "\n",
    "# 模型績效\n",
    "mse = np.mean((lm.predict(temperatures) - iced_tea_sales) ** 2)\n",
    "r_squared = lm.score(temperatures, iced_tea_sales)\n",
    "\n",
    "# 印出模型績效\n",
    "print(mse)\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 24.62695278],\n",
       "       [ 39.68444953],\n",
       "       [  5.16710978],\n",
       "       [ 20.13123193],\n",
       "       [  3.663767  ],\n",
       "       [ 64.60052107],\n",
       "       [ 10.5696598 ],\n",
       "       [ 20.36911739],\n",
       "       [ 21.64094005],\n",
       "       [ 16.08822702],\n",
       "       [ 37.03821634],\n",
       "       [ 42.42198083],\n",
       "       [ 17.44074211],\n",
       "       [ 67.64519009]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lm.predict(temperatures) - iced_tea_sales) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 複迴歸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 範例:利用店面面積（坪）與車站距離（公里）來預測分店單月銷售量（萬日圓）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 41.51347826  -0.34088269]\n",
      "65.3239163889\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#不用轉換維度\n",
    "X = np.array([\n",
    "    [10, 80], [8, 0], [8, 200], [5, 200], [7, 300], [8, 230], [7, 40], [9, 0], [6, 330], [9, 180]\n",
    "])\n",
    "y = np.array([469, 366, 371, 208, 246, 297, 363, 436, 198, 364])\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# 印出係數  have 2 val\n",
    "print(lm.coef_)\n",
    "\n",
    "# 印出截距\n",
    "print(lm.intercept_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 442.96160353]\n"
     ]
    }
   ],
   "source": [
    "#利用LinearRegression model+新蛋糕店資料 => 預測新蛋糕店的單月銷量  \n",
    "to_be_predicted = np.array([\n",
    "    [10, 110]\n",
    "])\n",
    "predicted_sales = lm.predict(to_be_predicted)\n",
    "\n",
    "print(predicted_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417.300611999\n",
      "0.945235852682\n",
      "0.929588953448\n"
     ]
    }
   ],
   "source": [
    "# 模型績效\n",
    "mse = np.mean((lm.predict(X) - y) ** 2)\n",
    "r_squared = lm.score(X, y)\n",
    "adj_r_squared = r_squared - (1 - r_squared) * (X.shape[1] / (X.shape[0] - X.shape[1] - 1))\n",
    "\n",
    "# 印出模型績效\n",
    "print(mse)   #均方差  均方跟誤差\n",
    "print(r_squared)   #自變項與依變項所形成的線性迴歸模式的契合度\n",
    "print(adj_r_squared)   #ADJUST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00051435  0.00844837]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "X = np.array([\n",
    "    [10, 80], [8, 0], [8, 200], [5, 200], [7, 300], [8, 230], [7, 40], [9, 0], [6, 330], [9, 180]\n",
    "])\n",
    "y = np.array([469, 366, 371, 208, 246, 297, 363, 436, 198, 364])\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# 印出 p-value\n",
    "print(f_regression(X, y)[1])    # H1 : 該項變數對(x)對y有顯著影響"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類（Classification）   預測類別目標變數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logistic 迴歸     (預測機率的方法)     \n",
    "#### 二元分類（Binary classification）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 範例:利用鐵達尼克號資料，使用 Sex，Pclass 與 Age 來預測 Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.98136537 -2.40629309 -0.023991  ]]\n",
      "[ 3.88051905]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, linear_model\n",
    "\n",
    "url = \"https://storage.googleapis.com/2017_ithome_ironman/data/kaggle_titanic_train.csv\"\n",
    "titanic_train = pd.read_csv(url)\n",
    "\n",
    "# 將 Age 遺漏值以 median 填補\n",
    "age_median = np.nanmedian(titanic_train[\"Age\"])    # 28 ;  np.mean(titanic_train[\"Age\"]) = 29.3615\n",
    "new_Age = np.where(titanic_train[\"Age\"].isnull(), age_median, titanic_train[\"Age\"])\n",
    "titanic_train[\"Age\"] = new_Age\n",
    "titanic_train\n",
    "\n",
    "# 創造 dummy variables   0v1\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "encoded_Sex = label_encoder.fit_transform(titanic_train[\"Sex\"])\n",
    "\n",
    "# 建立 train_X\n",
    "train_X = pd.DataFrame([titanic_train[\"Pclass\"],\n",
    "                        encoded_Sex,\n",
    "                        titanic_train[\"Age\"]\n",
    "]).T\n",
    "\n",
    "# 建立模型\n",
    "logistic_regr = linear_model.LogisticRegression()\n",
    "logistic_regr.fit(train_X, titanic_train[\"Survived\"])\n",
    "\n",
    "# 印出係數\n",
    "print(logistic_regr.coef_)\n",
    "\n",
    "# 印出截距\n",
    "print(logistic_regr.intercept_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.53704739e-25   1.40606613e-69   5.27606885e-02]\n",
      "0.796857463524\n",
      "0.796857463524\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# 印出 p-value\n",
    "print(f_regression(train_X, titanic_train[\"Survived\"])[1])\n",
    "\n",
    "# 計算準確率  use model.score(X,y)\n",
    "accuracy = logistic_regr.score(train_X, titanic_train[\"Survived\"])\n",
    "print(accuracy)\n",
    "\n",
    "# 計算準確率  use metrics.accuracy_score(test_y, test_y_predicted)\n",
    "survived_predictions = logistic_regr.predict(train_X)\n",
    "accuracy2 = metrics.accuracy_score(titanic_train[\"Survived\"], survived_predictions)\n",
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "train_X\n",
    "# print(logistic_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 決策樹分類器（Decision Tree Classifiers)-- Multiclass classification\n",
    "#### 決策樹分類器不需要將變數創造成 dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 0 2 1 1 0 0 0 1 2 2 2 2 2 1 0 2 1 2 0 0 0 0 2 1 2 0 1 1 2 2 2 1\n",
      " 2 0 0 1 0 2 1 0]\n",
      "[0 1 0 0 1 0 2 1 1 0 0 0 1 2 1 2 2 2 1 0 2 1 2 0 0 0 0 2 1 2 0 1 2 2 2 1 1\n",
      " 2 0 0 1 0 2 1 0]\n",
      "0.933333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# 讀入鳶尾花資料\n",
    "iris = load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "\n",
    "# 切分訓練與測試資料\n",
    "train_X, test_X, train_y, test_y = train_test_split(iris_X, iris_y, test_size = 0.3)\n",
    "\n",
    "# 建立分類器\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "iris_clf = clf.fit(train_X, train_y)\n",
    "\n",
    "# 預測\n",
    "test_y_predicted = iris_clf.predict(test_X)\n",
    "print(test_y_predicted)\n",
    "\n",
    "# 標準答案\n",
    "print(test_y)\n",
    "\n",
    "# 績效\n",
    "accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n',\n",
       " 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "        [ 4.9,  3. ,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.3,  0.2],\n",
       "        [ 4.6,  3.1,  1.5,  0.2],\n",
       "        [ 5. ,  3.6,  1.4,  0.2],\n",
       "        [ 5.4,  3.9,  1.7,  0.4],\n",
       "        [ 4.6,  3.4,  1.4,  0.3],\n",
       "        [ 5. ,  3.4,  1.5,  0.2],\n",
       "        [ 4.4,  2.9,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5.4,  3.7,  1.5,  0.2],\n",
       "        [ 4.8,  3.4,  1.6,  0.2],\n",
       "        [ 4.8,  3. ,  1.4,  0.1],\n",
       "        [ 4.3,  3. ,  1.1,  0.1],\n",
       "        [ 5.8,  4. ,  1.2,  0.2],\n",
       "        [ 5.7,  4.4,  1.5,  0.4],\n",
       "        [ 5.4,  3.9,  1.3,  0.4],\n",
       "        [ 5.1,  3.5,  1.4,  0.3],\n",
       "        [ 5.7,  3.8,  1.7,  0.3],\n",
       "        [ 5.1,  3.8,  1.5,  0.3],\n",
       "        [ 5.4,  3.4,  1.7,  0.2],\n",
       "        [ 5.1,  3.7,  1.5,  0.4],\n",
       "        [ 4.6,  3.6,  1. ,  0.2],\n",
       "        [ 5.1,  3.3,  1.7,  0.5],\n",
       "        [ 4.8,  3.4,  1.9,  0.2],\n",
       "        [ 5. ,  3. ,  1.6,  0.2],\n",
       "        [ 5. ,  3.4,  1.6,  0.4],\n",
       "        [ 5.2,  3.5,  1.5,  0.2],\n",
       "        [ 5.2,  3.4,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.6,  0.2],\n",
       "        [ 4.8,  3.1,  1.6,  0.2],\n",
       "        [ 5.4,  3.4,  1.5,  0.4],\n",
       "        [ 5.2,  4.1,  1.5,  0.1],\n",
       "        [ 5.5,  4.2,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5. ,  3.2,  1.2,  0.2],\n",
       "        [ 5.5,  3.5,  1.3,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 4.4,  3. ,  1.3,  0.2],\n",
       "        [ 5.1,  3.4,  1.5,  0.2],\n",
       "        [ 5. ,  3.5,  1.3,  0.3],\n",
       "        [ 4.5,  2.3,  1.3,  0.3],\n",
       "        [ 4.4,  3.2,  1.3,  0.2],\n",
       "        [ 5. ,  3.5,  1.6,  0.6],\n",
       "        [ 5.1,  3.8,  1.9,  0.4],\n",
       "        [ 4.8,  3. ,  1.4,  0.3],\n",
       "        [ 5.1,  3.8,  1.6,  0.2],\n",
       "        [ 4.6,  3.2,  1.4,  0.2],\n",
       "        [ 5.3,  3.7,  1.5,  0.2],\n",
       "        [ 5. ,  3.3,  1.4,  0.2],\n",
       "        [ 7. ,  3.2,  4.7,  1.4],\n",
       "        [ 6.4,  3.2,  4.5,  1.5],\n",
       "        [ 6.9,  3.1,  4.9,  1.5],\n",
       "        [ 5.5,  2.3,  4. ,  1.3],\n",
       "        [ 6.5,  2.8,  4.6,  1.5],\n",
       "        [ 5.7,  2.8,  4.5,  1.3],\n",
       "        [ 6.3,  3.3,  4.7,  1.6],\n",
       "        [ 4.9,  2.4,  3.3,  1. ],\n",
       "        [ 6.6,  2.9,  4.6,  1.3],\n",
       "        [ 5.2,  2.7,  3.9,  1.4],\n",
       "        [ 5. ,  2. ,  3.5,  1. ],\n",
       "        [ 5.9,  3. ,  4.2,  1.5],\n",
       "        [ 6. ,  2.2,  4. ,  1. ],\n",
       "        [ 6.1,  2.9,  4.7,  1.4],\n",
       "        [ 5.6,  2.9,  3.6,  1.3],\n",
       "        [ 6.7,  3.1,  4.4,  1.4],\n",
       "        [ 5.6,  3. ,  4.5,  1.5],\n",
       "        [ 5.8,  2.7,  4.1,  1. ],\n",
       "        [ 6.2,  2.2,  4.5,  1.5],\n",
       "        [ 5.6,  2.5,  3.9,  1.1],\n",
       "        [ 5.9,  3.2,  4.8,  1.8],\n",
       "        [ 6.1,  2.8,  4. ,  1.3],\n",
       "        [ 6.3,  2.5,  4.9,  1.5],\n",
       "        [ 6.1,  2.8,  4.7,  1.2],\n",
       "        [ 6.4,  2.9,  4.3,  1.3],\n",
       "        [ 6.6,  3. ,  4.4,  1.4],\n",
       "        [ 6.8,  2.8,  4.8,  1.4],\n",
       "        [ 6.7,  3. ,  5. ,  1.7],\n",
       "        [ 6. ,  2.9,  4.5,  1.5],\n",
       "        [ 5.7,  2.6,  3.5,  1. ],\n",
       "        [ 5.5,  2.4,  3.8,  1.1],\n",
       "        [ 5.5,  2.4,  3.7,  1. ],\n",
       "        [ 5.8,  2.7,  3.9,  1.2],\n",
       "        [ 6. ,  2.7,  5.1,  1.6],\n",
       "        [ 5.4,  3. ,  4.5,  1.5],\n",
       "        [ 6. ,  3.4,  4.5,  1.6],\n",
       "        [ 6.7,  3.1,  4.7,  1.5],\n",
       "        [ 6.3,  2.3,  4.4,  1.3],\n",
       "        [ 5.6,  3. ,  4.1,  1.3],\n",
       "        [ 5.5,  2.5,  4. ,  1.3],\n",
       "        [ 5.5,  2.6,  4.4,  1.2],\n",
       "        [ 6.1,  3. ,  4.6,  1.4],\n",
       "        [ 5.8,  2.6,  4. ,  1.2],\n",
       "        [ 5. ,  2.3,  3.3,  1. ],\n",
       "        [ 5.6,  2.7,  4.2,  1.3],\n",
       "        [ 5.7,  3. ,  4.2,  1.2],\n",
       "        [ 5.7,  2.9,  4.2,  1.3],\n",
       "        [ 6.2,  2.9,  4.3,  1.3],\n",
       "        [ 5.1,  2.5,  3. ,  1.1],\n",
       "        [ 5.7,  2.8,  4.1,  1.3],\n",
       "        [ 6.3,  3.3,  6. ,  2.5],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 7.1,  3. ,  5.9,  2.1],\n",
       "        [ 6.3,  2.9,  5.6,  1.8],\n",
       "        [ 6.5,  3. ,  5.8,  2.2],\n",
       "        [ 7.6,  3. ,  6.6,  2.1],\n",
       "        [ 4.9,  2.5,  4.5,  1.7],\n",
       "        [ 7.3,  2.9,  6.3,  1.8],\n",
       "        [ 6.7,  2.5,  5.8,  1.8],\n",
       "        [ 7.2,  3.6,  6.1,  2.5],\n",
       "        [ 6.5,  3.2,  5.1,  2. ],\n",
       "        [ 6.4,  2.7,  5.3,  1.9],\n",
       "        [ 6.8,  3. ,  5.5,  2.1],\n",
       "        [ 5.7,  2.5,  5. ,  2. ],\n",
       "        [ 5.8,  2.8,  5.1,  2.4],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 6.5,  3. ,  5.5,  1.8],\n",
       "        [ 7.7,  3.8,  6.7,  2.2],\n",
       "        [ 7.7,  2.6,  6.9,  2.3],\n",
       "        [ 6. ,  2.2,  5. ,  1.5],\n",
       "        [ 6.9,  3.2,  5.7,  2.3],\n",
       "        [ 5.6,  2.8,  4.9,  2. ],\n",
       "        [ 7.7,  2.8,  6.7,  2. ],\n",
       "        [ 6.3,  2.7,  4.9,  1.8],\n",
       "        [ 6.7,  3.3,  5.7,  2.1],\n",
       "        [ 7.2,  3.2,  6. ,  1.8],\n",
       "        [ 6.2,  2.8,  4.8,  1.8],\n",
       "        [ 6.1,  3. ,  4.9,  1.8],\n",
       "        [ 6.4,  2.8,  5.6,  2.1],\n",
       "        [ 7.2,  3. ,  5.8,  1.6],\n",
       "        [ 7.4,  2.8,  6.1,  1.9],\n",
       "        [ 7.9,  3.8,  6.4,  2. ],\n",
       "        [ 6.4,  2.8,  5.6,  2.2],\n",
       "        [ 6.3,  2.8,  5.1,  1.5],\n",
       "        [ 6.1,  2.6,  5.6,  1.4],\n",
       "        [ 7.7,  3. ,  6.1,  2.3],\n",
       "        [ 6.3,  3.4,  5.6,  2.4],\n",
       "        [ 6.4,  3.1,  5.5,  1.8],\n",
       "        [ 6. ,  3. ,  4.8,  1.8],\n",
       "        [ 6.9,  3.1,  5.4,  2.1],\n",
       "        [ 6.7,  3.1,  5.6,  2.4],\n",
       "        [ 6.9,  3.1,  5.1,  2.3],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 6.8,  3.2,  5.9,  2.3],\n",
       "        [ 6.7,  3.3,  5.7,  2.5],\n",
       "        [ 6.7,  3. ,  5.2,  2.3],\n",
       "        [ 6.3,  2.5,  5. ,  1.9],\n",
       "        [ 6.5,  3. ,  5.2,  2. ],\n",
       "        [ 6.2,  3.4,  5.4,  2.3],\n",
       "        [ 5.9,  3. ,  5.1,  1.8]]),\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], \n",
       "       dtype='<U10')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors 分類器    --   Multiclass classification\n",
    "#### 以距離作為未知類別的資料點分類依據，必須要將類別變數轉換為 dummy variables 然後將所有的數值型變數標準化，避免因為單位不同，在距離的計算上失真。\n",
    "#### 預設 k = 5    ;   k 的上限為訓練樣本數的 20%。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 2 0 1 0 1 2 2 2 0 0 0 0 2 0 2 2 2 0 1 1 1 2 0 0 2 1 0 1 2 0 0 1 1 0\n",
      " 2 2 2 1 2 2 0 0]\n",
      "[0 0 2 2 0 1 0 1 2 2 2 0 0 0 0 2 0 2 2 2 0 1 1 1 2 0 0 1 1 0 1 2 0 0 1 1 0\n",
      " 2 2 2 1 2 2 0 0]\n",
      "0.977777777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import neighbors\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# 讀入鳶尾花資料\n",
    "iris = load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "\n",
    "# 切分訓練與測試資料\n",
    "train_X, test_X, train_y, test_y = train_test_split(iris_X, iris_y, test_size = 0.3)\n",
    "\n",
    "# 建立分類器\n",
    "clf = neighbors.KNeighborsClassifier()\n",
    "iris_clf = clf.fit(train_X, train_y)\n",
    "\n",
    "# 預測\n",
    "test_y_predicted = iris_clf.predict(test_X)\n",
    "print(test_y_predicted)\n",
    "\n",
    "# 標準答案\n",
    "print(test_y)\n",
    "print(metrics.accuracy_score(test_y, test_y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL2E1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21fb712ec88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0.955555555556\n"
     ]
    }
   ],
   "source": [
    "#k 在介於 8 到 12 之間模型的準確率最高\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import neighbors\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 讀入鳶尾花資料\n",
    "iris = load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "\n",
    "# 切分訓練與測試資料\n",
    "train_X, test_X, train_y, test_y = train_test_split(iris_X, iris_y, test_size = 0.3)\n",
    "\n",
    "# 選擇 k\n",
    "range = np.arange(1, round(0.2 * train_X.shape[0]) + 1)     ##\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "#use for loop find maxAccuracies of data classification(n_neighbors = i)距離\n",
    "for i in range:  \n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors = i)    # model(k)\n",
    "    iris_clf = clf.fit(train_X, train_y)\n",
    "    test_y_predicted = iris_clf.predict(test_X)\n",
    "    accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# 視覺化\n",
    "plt.scatter(range, accuracies)\n",
    "plt.show()\n",
    "appr_k = accuracies.index(max(accuracies)) + 1  # find k of max(accuracies)\n",
    "print(appr_k)\n",
    "print(max(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非監督式(unsupervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分群演算法 ( K-Means 與 Hierarchical Clustering)\n",
    "#### 分群演算法的績效衡量簡單明暸：組間差異大，組內差異小。而所謂的差異指的就是觀測值之間的距離遠近作為衡量，最常見還是使用歐氏距離（Euclidean distance）。既然我們又是以距離作為度量，在資料的預處理程序中，與 k-Nearest Neighbors 分類器一樣我們必須將所有的數值型變數標準化（Normalization），避免因為單位不同，在距離的計算上失真。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means\n",
    "#### K-Means 演算法可以非常快速地完成分群任務，但是如果觀測值具有雜訊（Noise）或者極端值，其分群結果容易被這些雜訊與極端值影響，適合處理分布集中的大型樣本資料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分群結果：\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0\n",
      " 0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0\n",
      " 0 2]\n",
      "---\n",
      "真實品種：\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster, datasets\n",
    "\n",
    "# 讀入鳶尾花資料\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "\n",
    "# KMeans 演算法\n",
    "kmeans_fit = cluster.KMeans(n_clusters = 3).fit(iris_X)     # k\n",
    "\n",
    "# 印出分群結果\n",
    "cluster_labels = kmeans_fit.labels_\n",
    "print(\"分群結果：\")\n",
    "print(cluster_labels)\n",
    "print(\"---\")\n",
    "\n",
    "# 印出品種看看\n",
    "iris_y = iris.target\n",
    "print(\"真實品種：\")\n",
    "print(iris_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.552591944521\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster, datasets, metrics\n",
    "\n",
    "# 讀入鳶尾花資料\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "\n",
    "# KMeans 演算法\n",
    "kmeans_fit = cluster.KMeans(n_clusters = 3).fit(iris_X)    # k\n",
    "# 分群結果\n",
    "cluster_labels = kmeans_fit.labels_\n",
    "\n",
    "# 印出績效  分群演算法的績效使用 Silhouette 係數\n",
    "silhouette_avg = metrics.silhouette_score(iris_X, cluster_labels)\n",
    "print(silhouette_avg)      #-1~1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b2ba88a320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.68081362027135062, 0.55259194452136751, 0.49782569007544908, 0.48851755085386289, 0.36820569653764568, 0.36220099688176, 0.35912838649346956, 0.33607614949225745, 0.33185320016692499]\n"
     ]
    }
   ],
   "source": [
    "#隨著 k 值的增加，K-Means 演算法的績效一定會愈來愈好\n",
    "#當 k = 觀測值數目的時候，我們會得到一個組間差異最大，組內差異最小的結果\n",
    "#但這不是我們想要的，實務上我們讓程式幫忙選擇一個適合的 k。\n",
    "\n",
    "from sklearn import cluster, datasets, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 讀入鳶尾花資料\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "\n",
    "# 迴圈\n",
    "silhouette_avgs = []\n",
    "\n",
    "ks = range(2, 11)        ## k of range\n",
    "for k in ks:\n",
    "    kmeans_fit = cluster.KMeans(n_clusters = k).fit(iris_X)\n",
    "    cluster_labels = kmeans_fit.labels_                   # 分群結果\n",
    "    silhouette_avg = metrics.silhouette_score(iris_X, cluster_labels) #績效 : 使用 Silhouette 係數\n",
    "    silhouette_avgs.append(silhouette_avg)\n",
    "\n",
    "# 作圖並印出 k = 2 到 10 的績效\n",
    "plt.bar(ks, silhouette_avgs)\n",
    "plt.show()\n",
    "print(silhouette_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#k值在等於 2 與 3 的時候 K-Means 演算法的績效較好\n",
    "#手肘點（Elbow point）出現在 k = 2 或者 k = 3 的時候\n",
    "#這也驗證了我們先前的觀察，setosa 這個品種跟另外兩個品種在花瓣（Petal）的長和寬跟花萼（Sepal）的長和寬有比較大的差異\n",
    "#因此如果是以 K-Means 分群，可能會將 setosa 歸為一群，versicolor 和 virginica 歸為一群。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "#### 與 K-Means 演算法不同的地方在於不需要事先設定 k 值，Hierarchical Clustering 演算法每一次只將兩個觀測值歸為一類，然後在演算過程中得到 k = 1 一直到 k = n（觀測值個數）群的結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n",
      " 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 0 2 2 2 0 2 2 2 0 2 2 2 0 2\n",
      " 2 0]\n",
      "---\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "0.554097290787\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cluster, datasets, metrics\n",
    "\n",
    "# 讀入鳶尾花資料\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "\n",
    "# Hierarchical Clustering 演算法     先用euclidean計算距離 再用ward分群(初始群到合併群中心最短的距離)\n",
    "hclust = cluster.AgglomerativeClustering(linkage = 'ward', affinity = 'euclidean', n_clusters = 3)\n",
    "#  {“ward”, “complete”, “average”}\n",
    "\n",
    "# 印出分群結果\n",
    "hclust_fit = hclust.fit(iris_X)\n",
    "cluster_labels = hclust_fit.labels_  # 分群結果\n",
    "print(cluster_labels)\n",
    "print(\"---\")\n",
    "\n",
    "# 印出品種看看\n",
    "iris_y = iris.target\n",
    "print(iris_y)\n",
    "\n",
    "# 印出績效  silhouette_score評估分群的績效\n",
    "silhouette_avg = metrics.silhouette_score(iris_X, cluster_labels)\n",
    "print(silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 以上是監督與非監督"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在經典的整體學習（Ensemble learning）演算法 Bagging 與 AdaBoost 中我們多數使用\"決策樹\"作為\"基本分類器\"\n",
    "#### 整體學習（Ensemble learning）可以將數個分類器的預測結果綜合考慮，藉此達到顯著提升分類效果\n",
    "### Bagging 是 Bootstrap Aggregating 的簡稱，透過統計學的 Bootstrap sampling 得到不同的訓練資料，然後根據這些訓練資料得到一系列的基本分類器，假如演算法產生了 5 個基本分類器，她們對某個觀測值的預測結果分別為 1, 0, 1, 1, 1，那麼 Bagging 演算法的輸出結果就會是 1，這個過程稱之為基本分類器的投票"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h4587\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794776119403\n"
     ]
    }
   ],
   "source": [
    "# BaggingClassifier 決策樹進化版\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation, ensemble, preprocessing, metrics\n",
    "\n",
    "# 載入資料\n",
    "url = \"https://storage.googleapis.com/2017_ithome_ironman/data/kaggle_titanic_train.csv\"\n",
    "titanic_train = pd.read_csv(url)\n",
    "\n",
    "# 填補遺漏值\n",
    "age_median = np.nanmedian(titanic_train[\"Age\"])\n",
    "new_Age = np.where(titanic_train[\"Age\"].isnull(), age_median, titanic_train[\"Age\"])\n",
    "titanic_train[\"Age\"] = new_Age\n",
    "\n",
    "# 創造 dummy variables\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "encoded_Sex = label_encoder.fit_transform(titanic_train[\"Sex\"])\n",
    "\n",
    "# 建立訓練與測試資料\n",
    "titanic_X = pd.DataFrame([titanic_train[\"Pclass\"],\n",
    "                         encoded_Sex,\n",
    "                         titanic_train[\"Age\"]\n",
    "]).T\n",
    "titanic_y = titanic_train[\"Survived\"]\n",
    "train_X, test_X, train_y, test_y = cross_validation.train_test_split(titanic_X, titanic_y, test_size = 0.3)\n",
    "\n",
    "# 建立 bagging 模型\n",
    "bag = ensemble.BaggingClassifier(n_estimators = 100)\n",
    "bag_fit = bag.fit(train_X, train_y)\n",
    "\n",
    "# 預測\n",
    "test_y_predicted = bag.predict(test_X)\n",
    "\n",
    "# 績效\n",
    "accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost 同樣是基於數個基本分類器的整體學習演算法，跟前述 Bagging 演算法不同的地方在於，她在形成基本分類器時除了隨機生成，還會針對在前一個基本分類器中被分類錯誤的觀測值提高抽樣權重，使得該觀測值在下一個基本分類器形成時有更高機率被選入，藉此提高被正確分類的機率，簡單來說，她是個具有即時調節觀測值抽樣權重的進階 Bagging 演算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.761194029851\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation, ensemble, preprocessing, metrics\n",
    "\n",
    "# 載入資料\n",
    "url = \"https://storage.googleapis.com/2017_ithome_ironman/data/kaggle_titanic_train.csv\"\n",
    "titanic_train = pd.read_csv(url)\n",
    "\n",
    "# 填補遺漏值\n",
    "age_median = np.nanmedian(titanic_train[\"Age\"])\n",
    "new_Age = np.where(titanic_train[\"Age\"].isnull(), age_median, titanic_train[\"Age\"])\n",
    "titanic_train[\"Age\"] = new_Age\n",
    "\n",
    "# 創造 dummy variables\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "encoded_Sex = label_encoder.fit_transform(titanic_train[\"Sex\"])\n",
    "\n",
    "# 建立訓練與測試資料\n",
    "titanic_X = pd.DataFrame([titanic_train[\"Pclass\"],\n",
    "                         encoded_Sex,\n",
    "                         titanic_train[\"Age\"]\n",
    "]).T\n",
    "titanic_y = titanic_train[\"Survived\"]\n",
    "train_X, test_X, train_y, test_y = cross_validation.train_test_split(titanic_X, titanic_y, test_size = 0.3)\n",
    "\n",
    "# 建立 boosting 模型\n",
    "boost = ensemble.AdaBoostClassifier(n_estimators = 100)\n",
    "boost_fit = boost.fit(train_X, train_y)\n",
    "\n",
    "# 預測\n",
    "test_y_predicted = boost.predict(test_X)\n",
    "\n",
    "# 績效\n",
    "accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
