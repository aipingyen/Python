{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "#處理編碼的套件\n",
    "import operator\n",
    "##處理字典檔排序的套件\n",
    "\n",
    "\n",
    "cutlist = ['!','\\xa3','\\xa2','%','$',\"'\",'&', ')','(','+','*','-','/', ':','=','<','?','>','@','\\xc2','#', '\"','[',']','\\\\', '_', '^', '`', '{', '}', '|', '~','–','!!','?!','??','!?','`','``',\"''\",',',':',\n",
    "';','\"',\"'\",'<','>','*','..','...','..','？']\n",
    "\n",
    "# \"<>/:：;；,、＂’，.。！？｢\\\"\\'\\\\\\n\\r《》“”!@#$%^&*()\".encode(\"utf-8\")  ##列出標點符號，並轉換成utf-8的格式\n",
    "\n",
    "\n",
    "def cutSentence(text_path, keywords): ##放入原始文章路徑, 增加斷詞的list\n",
    "    text = codecs.open(text_path,\"r\",\"utf-8\")   #開檔\n",
    "    sentence = \"\"\n",
    "    textList = []\n",
    "       \n",
    "    for line in text.readlines():\n",
    "        line = line.strip() ##清除空白\n",
    "        \n",
    "        for keyword in keywords:  #清除關鍵字\n",
    "            line = \"\".join(line.split(keyword))\n",
    "            \n",
    "        for word in line:\n",
    "            if word not in cutlist: #如果文字不是標點符號，就把字加到句子中\n",
    "                sentence += word\n",
    "                #print sentence\n",
    "            else:\n",
    "                textList.append(sentence) #如果遇到標點符號，把句子加到 text list中\n",
    "                sentence = \"\"\n",
    "                #print textList\n",
    "    return textList#傳回一個文字陣列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(textLists,n,minFreq): #第一個參數放處理好的文章(LIST檔，utf-8編碼)，第二個參數放字詞的長度單位，第三個參數放至少要幾次以上\n",
    " \n",
    "    words=[]     #存放擷取出來的字詞\n",
    "    words_freq={}#存放字詞:計算個數 \n",
    "    result= []\n",
    "    for textList in textLists:\n",
    "        for w in range(len(textList)-(n-1)): #要讀取的長度隨字詞長度改變\n",
    "            words.append(textList[w:w+n])    #抓取長度w-(n-1)的字串\n",
    "\n",
    "    for word in words:\n",
    "        if word not in words_freq:               #如果這個字詞還沒有被放在字典檔中\n",
    "            words_freq[word] = words.count(word) #就開一個新的字詞，裡面放入字詞計算的頻次\n",
    " \n",
    "    words_freq = sorted(words_freq.iteritems(),key=operator.itemgetter(1),reverse=True) #change words_freq from dict to list \n",
    "    \n",
    "    for word in words_freq:\n",
    "        if word[1] >= minFreq:\n",
    "            result.append(word)\n",
    "            \n",
    "    return result ##回傳一個陣列[詞,頻次]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def longTermPriority(path, maxTermLength, minFreq):\n",
    "    longTerms=[]          #長詞\n",
    "    longTermsFreq=[]      #長詞+次數分配\n",
    "    \n",
    "    for i in range(maxTermLength,1,-1): ##字詞數由大至小\n",
    "        text_list = cutSentence(path,longTerms)  #呼叫cutSentence function\n",
    "        #print len(text_list)\n",
    "        words_freq = ngram(text_list,i, minFreq) #呼叫 ngram function\n",
    "        #print i\n",
    "\n",
    "    \n",
    "        for word_freq in words_freq:\n",
    "            longTerms.append(word_freq[0])  #將跑出來的長詞加入 longTerms list 做為下次切割檔案的基礎\n",
    "            #print word_freq[0]\n",
    "            longTermsFreq.append(word_freq) #將長詞和次數加入另外一個list  分成兩個檔儲存的用意是減少迴圈次數\n",
    "            #print word_freq\n",
    "    \n",
    "    return longTermsFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "#處理編碼的套件\n",
    "import operator\n",
    "##處理字典檔排序的套件\n",
    " \n",
    "cutlist = ['!','\\xa3','\\xa2','%','$',\"'\",'&', ')','(','+','*','-','/', ':','=','<','?','>','@','\\xc2','#', '\"','[',']','\\\\', '_', '^', '`', '{', '}', '|', '~','–','!!','?!','??','!?','`','``',\"''\",',',':',\n",
    "';','\"',\"'\",'<','>','*','..','...','..','？']\n",
    "# cutlist = \"<>/:：;；,、＂’，.。！？｢\\\"\\'\\\\\\n\\r《》“”!@#$%^&*()\".decode(\"utf-8\")  \n",
    "\n",
    "\n",
    "text_new =\"\"\n",
    "\n",
    "\n",
    "def cutSentence(text_path, keywords): ##放入原始文章路徑, 增加斷詞的list\n",
    "    text = codecs.open(text_path,\"r\",\"utf-8\")   #開檔 #讀取存成TXT檔的文字，讀入後統一轉成UTF-8格式\n",
    "    sentence = \"\"\n",
    "    textList = []\n",
    "       \n",
    "    for line in text.readlines():\n",
    "        line = line.strip() ##清除空白\n",
    "        \n",
    "        for keyword in keywords:  #清除關鍵字\n",
    "            line = \"\".join(line.split(keyword))\n",
    "            \n",
    "        for word in line:\n",
    "            if word not in cutlist: #如果文字不是標點符號，就把字加到句子中\n",
    "                sentence += word\n",
    "                #print sentence\n",
    "            else:\n",
    "                textList.append(sentence) #如果遇到標點符號，把句子加到 text list中\n",
    "                sentence = \"\"\n",
    "                #print textList\n",
    "    return textList#傳回一個文字陣列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(textLists,n,minFreq): #第一個參數放處理好的文章(LIST檔，utf-8編碼)，第二個參數放字詞的長度單位，第三個參數放至少要幾次以上\n",
    " \n",
    "    words=[]     #存放擷取出來的字詞\n",
    "    words_freq={}#存放字詞:計算個數 \n",
    "    result= []\n",
    "    for textList in textLists:\n",
    "        for w in range(len(textList)-(n-1)): #要讀取的長度隨字詞長度改變\n",
    "            words.append(textList[w:w+n]) #抓取長度w-(n-1)的字串\n",
    "\n",
    "    for word in words:\n",
    "        if word not in words_freq:               #如果這個字詞還沒有被放在字典檔中\n",
    "            words_freq[word] = words.count(word) #就開一個新的字詞，裡面放入字詞計算的頻次\n",
    " \n",
    "    words_freq = sorted(words_freq.items(),key=operator.itemgetter(1),reverse=True) #change words_freq from dict to list \n",
    "    \n",
    "    for word in words_freq:\n",
    "        if word[1] >= minFreq:\n",
    "            result.append(word)\n",
    "            \n",
    "    return result ##回傳一個陣列[詞,頻次]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def longTermPriority(path, maxTermLength, minFreq):\n",
    "    longTerms=[]          #長詞\n",
    "    longTermsFreq=[]      #長詞+次數分配\n",
    "    \n",
    "    for i in range(maxTermLength,1,-1):\n",
    "        text_list = cutSentence(path,longTerms)\n",
    "        #print len(text_list)\n",
    "        words_freq = ngram(text_list,i, minFreq)\n",
    "        #print i\n",
    "\n",
    "    \n",
    "        for word_freq in words_freq:\n",
    "            longTerms.append(word_freq[0])\n",
    "            #print word_freq[0]\n",
    "            longTermsFreq.append(word_freq) \n",
    "            #print word_freq\n",
    "    \n",
    "    return longTermsFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa6 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c951eaf634db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlongTermFreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlongTermPriority\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"E:\\\\PythonWin\\\\workspace\\\\equip.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##最長詞6個字、出現頻次5次以上\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu\"E:\\\\PythonWin\\\\workspace\\\\1Wd.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlongTermFreq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{},{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-b82889db27a9>\u001b[0m in \u001b[0;36mlongTermPriority\u001b[1;34m(path, maxTermLength, minFreq)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxTermLength\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtext_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcutSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlongTerms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;31m#print len(text_list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mwords_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminFreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-03626ff5ce4c>\u001b[0m in \u001b[0;36mcutSentence\u001b[1;34m(text_path, keywords)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mtextList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##清除空白\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\pythonwin\\python36\\lib\\codecs.py\u001b[0m in \u001b[0;36mreadlines\u001b[1;34m(self, sizehint)\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msizehint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msizehint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\pythonwin\\python36\\lib\\codecs.py\u001b[0m in \u001b[0;36mreadlines\u001b[1;34m(self, sizehint, keepends)\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \"\"\"\n\u001b[1;32m--> 615\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeepends\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\pythonwin\\python36\\lib\\codecs.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size, chars, firstline)\u001b[0m\n\u001b[0;32m    499\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m                 \u001b[0mnewchars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecodedbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfirstline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xa6 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "longTermFreq = longTermPriority(\"E:\\\\PythonWin\\\\workspace\\\\equip.txt\",5,1) ##最長詞6個字、出現頻次5次以上\n",
    "with open(u\"E:\\\\PythonWin\\\\workspace\\\\1Wd.csv\",'w') as fw:\n",
    "    for k,v in longTermFreq:\n",
    "        fw.write(\"{},{}\".format(k,v)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
