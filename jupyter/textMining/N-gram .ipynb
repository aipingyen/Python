{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "#處理編碼的套件\n",
    "import operator\n",
    "##處理字典檔排序的套件\n",
    "\n",
    "\n",
    "cutlist = ['!','\\xa3','\\xa2','%','$',\"'\",'&', ')','(','+','*','-','/', ':','=','<','?','>','@','\\xc2','#', '\"','[',']','\\\\', '_', '^', '`', '{', '}', '|', '~','–','!!','?!','??','!?','`','``',\"''\",',',':',\n",
    "';','\"',\"'\",'<','>','*','..','...','..']\n",
    "\n",
    "# \"<>/:：;；,、＂’，.。！？｢\\\"\\'\\\\\\n\\r《》“”!@#$%^&*()\".encode(\"utf-8\")  ##列出標點符號，並轉換成utf-8的格式\n",
    "\n",
    "\n",
    "def cutSentence(text_path, keywords): ##放入原始文章路徑, 增加斷詞的list\n",
    "    text = codecs.open(text_path,\"r\",\"utf-8\")   #開檔\n",
    "    sentence = \"\"\n",
    "    textList = []\n",
    "       \n",
    "    for line in text.readlines():\n",
    "        line = line.strip() ##清除空白\n",
    "        \n",
    "        for keyword in keywords:  #清除關鍵字\n",
    "            line = \"\".join(line.split(keyword))\n",
    "            \n",
    "        for word in line:\n",
    "            if word not in cutlist: #如果文字不是標點符號，就把字加到句子中\n",
    "                sentence += word\n",
    "                #print sentence\n",
    "            else:\n",
    "                textList.append(sentence) #如果遇到標點符號，把句子加到 text list中\n",
    "                sentence = \"\"\n",
    "                #print textList\n",
    "    return textList#傳回一個文字陣列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(textLists,n,minFreq): #第一個參數放處理好的文章(LIST檔，utf-8編碼)，第二個參數放字詞的長度單位，第三個參數放至少要幾次以上\n",
    " \n",
    "    words=[]     #存放擷取出來的字詞\n",
    "    words_freq={}#存放字詞:計算個數 \n",
    "    result= []\n",
    "    for textList in textLists:\n",
    "        for w in range(len(textList)-(n-1)): #要讀取的長度隨字詞長度改變\n",
    "            words.append(textList[w:w+n])    #抓取長度w-(n-1)的字串\n",
    "\n",
    "    for word in words:\n",
    "        if word not in words_freq:               #如果這個字詞還沒有被放在字典檔中\n",
    "            words_freq[word] = words.count(word) #就開一個新的字詞，裡面放入字詞計算的頻次\n",
    " \n",
    "    words_freq = sorted(words_freq.iteritems(),key=operator.itemgetter(1),reverse=True) #change words_freq from dict to list \n",
    "    \n",
    "    for word in words_freq:\n",
    "        if word[1] >= minFreq:\n",
    "            result.append(word)\n",
    "            \n",
    "    return result ##回傳一個陣列[詞,頻次]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def longTermPriority(path, maxTermLength, minFreq):\n",
    "    longTerms=[]          #長詞\n",
    "    longTermsFreq=[]      #長詞+次數分配\n",
    "    \n",
    "    for i in range(maxTermLength,1,-1): ##字詞數由大至小\n",
    "        text_list = cutSentence(path,longTerms)  #呼叫cutSentence function\n",
    "        #print len(text_list)\n",
    "        words_freq = ngram(text_list,i, minFreq) #呼叫 ngram function\n",
    "        #print i\n",
    "\n",
    "    \n",
    "        for word_freq in words_freq:\n",
    "            longTerms.append(word_freq[0])  #將跑出來的長詞加入 longTerms list 做為下次切割檔案的基礎\n",
    "            #print word_freq[0]\n",
    "            longTermsFreq.append(word_freq) #將長詞和次數加入另外一個list  分成兩個檔儲存的用意是減少迴圈次數\n",
    "            #print word_freq\n",
    "    \n",
    "    return longTermsFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "#處理編碼的套件\n",
    "import operator\n",
    "##處理字典檔排序的套件\n",
    " \n",
    "cutlist = ['!','\\xa3','\\xa2','%','$',\"'\",'&', ')','(','+','*','-','/', ':','=','<','?','>','@','\\xc2','#', '\"','[',']','\\\\', '_', '^', '`', '{', '}', '|', '~','–','!!','?!','??','!?','`','``',\"''\",',',':',\n",
    "';','\"',\"'\",'<','>','*','..','...','..']\n",
    "# cutlist = \"<>/:：;；,、＂’，.。！？｢\\\"\\'\\\\\\n\\r《》“”!@#$%^&*()\".decode(\"utf-8\")  \n",
    "\n",
    "\n",
    "text_new =\"\"\n",
    "\n",
    "\n",
    "def cutSentence(text_path, keywords): ##放入原始文章路徑, 增加斷詞的list\n",
    "    text = codecs.open(text_path,\"r\",\"utf-8\")   #開檔 #讀取存成TXT檔的文字，讀入後統一轉成UTF-8格式\n",
    "    sentence = \"\"\n",
    "    textList = []\n",
    "       \n",
    "    for line in text.readlines():\n",
    "        line = line.strip() ##清除空白\n",
    "        \n",
    "        for keyword in keywords:  #清除關鍵字\n",
    "            line = \"\".join(line.split(keyword))\n",
    "            \n",
    "        for word in line:\n",
    "            if word not in cutlist: #如果文字不是標點符號，就把字加到句子中\n",
    "                sentence += word\n",
    "                #print sentence\n",
    "            else:\n",
    "                textList.append(sentence) #如果遇到標點符號，把句子加到 text list中\n",
    "                sentence = \"\"\n",
    "                #print textList\n",
    "    return textList#傳回一個文字陣列\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram(textLists,n,minFreq): #第一個參數放處理好的文章(LIST檔，utf-8編碼)，第二個參數放字詞的長度單位，第三個參數放至少要幾次以上\n",
    " \n",
    "    words=[]     #存放擷取出來的字詞\n",
    "    words_freq={}#存放字詞:計算個數 \n",
    "    result= []\n",
    "    for textList in textLists:\n",
    "        for w in range(len(textList)-(n-1)): #要讀取的長度隨字詞長度改變\n",
    "            words.append(textList[w:w+n]) #抓取長度w-(n-1)的字串\n",
    "\n",
    "    for word in words:\n",
    "        if word not in words_freq:               #如果這個字詞還沒有被放在字典檔中\n",
    "            words_freq[word] = words.count(word) #就開一個新的字詞，裡面放入字詞計算的頻次\n",
    " \n",
    "    words_freq = sorted(words_freq.items(),key=operator.itemgetter(1),reverse=True) #change words_freq from dict to list \n",
    "    \n",
    "    for word in words_freq:\n",
    "        if word[1] >= minFreq:\n",
    "            result.append(word)\n",
    "            \n",
    "    return result ##回傳一個陣列[詞,頻次]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def longTermPriority(path, maxTermLength, minFreq):\n",
    "    longTerms=[]          #長詞\n",
    "    longTermsFreq=[]      #長詞+次數分配\n",
    "    \n",
    "    for i in range(maxTermLength,1,-1):\n",
    "        text_list = cutSentence(path,longTerms)\n",
    "        #print len(text_list)\n",
    "        words_freq = ngram(text_list,i, minFreq)\n",
    "        #print i\n",
    "\n",
    "    \n",
    "        for word_freq in words_freq:\n",
    "            longTerms.append(word_freq[0])\n",
    "            #print word_freq[0]\n",
    "            longTermsFreq.append(word_freq) \n",
    "            #print word_freq\n",
    "    \n",
    "    return longTermsFreq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BMW X3  3\n",
      "智能領航版 4\n",
      "台灣看世界 3\n",
      "730d 3\n",
      "BMW 4\n",
      "0左右 3\n",
      "400 3\n",
      "7er 3\n",
      "350 3\n",
      "真的 10\n",
      "後座 6\n",
      "自駕 6\n",
      "因為 5\n",
      "科技 5\n",
      "配備 5\n",
      "20 4\n",
      "業代 4\n",
      ".. 4\n",
      "S咖 4\n",
      "大概 4\n",
      "多了 4\n",
      "包含 4\n",
      "等等 4\n",
      "出廠 3\n",
      "可以 3\n",
      "7也 3\n",
      "智能 3\n",
      "柴油 3\n",
      "看車 3\n",
      "這次 3\n",
      "並且 3\n",
      "比較 3\n",
      "套件 3\n",
      "嗎？ 3\n",
      "駕駛 3\n",
      "還是 3\n",
      "服務 3\n",
      "台灣 3\n"
     ]
    }
   ],
   "source": [
    "longTermFreq = longTermPriority(\"/home/iii/Downloads/text.txt\",15,3) ##最長詞6個字、出現頻次5次以上\n",
    "\n",
    "\n",
    "for k,v in longTermFreq:\n",
    "    print (k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
