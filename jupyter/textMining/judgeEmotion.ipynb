{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整體輿論為負面\n",
      "出現正面詞彙的段落:[]\n",
      "出現負面詞彙的段落:['只可惜只有日本才看得到']\n",
      "---------------------------------------------\n",
      "positive keyword:[]\n",
      "negative keyword:['可惜']\n",
      "=============================================\n",
      "整體輿論為中性\n",
      "出現正面詞彙的段落:['好像日系車包括toyota', '是否還有其他人有聽說呢']\n",
      "出現負面詞彙的段落:['那明年買車不就少了5萬空間', '會不會太誇張']\n",
      "---------------------------------------------\n",
      "positive keyword:['還有', '聽說', '好像']\n",
      "negative keyword:['不會', '誇張', '不就']\n",
      "=============================================\n",
      "整體輿論為正面\n",
      "出現正面詞彙的段落:['😀今天去展示場去看了實車有點失望以下心得1.1.6Livina 外形 太楊春  感覺誠意不夠  只有頂級才有外包2.1.6Livina 49.9萬 跟 1.6 TIIDA 49.9  比皮椅 TIIDA 優  比空間 TIIDA 優  比組裝  TIIDA 優    比質感 TIIDA 勝  比油耗  TIIDA 勝   比折價 TIIDA 優  約3']\n",
      "出現負面詞彙的段落:['😀今天去展示場去看了實車有點失望以下心得1.1.6Livina 外形 太楊春  感覺誠意不夠  只有頂級才有外包2.1.6Livina 49.9萬 跟 1.6 TIIDA 49.9  比皮椅 TIIDA 優  比空間 TIIDA 優  比組裝  TIIDA 優    比質感 TIIDA 勝  比油耗  TIIDA 勝   比折價 TIIDA 優  約3']\n",
      "---------------------------------------------\n",
      "positive keyword:['感覺', '誠意']\n",
      "negative keyword:['不夠']\n",
      "=============================================\n",
      "整體輿論為正面\n",
      "出現正面詞彙的段落:['這兩個是我比較想要的', '6.加滿油以上63.9萬', '板上如果有更優點的單', '也許需等廠商開模.量身訂做吧', '謝謝', '保險.動保.領牌 實報實銷還有很多常見的配備 例', '防水托盤.避光墊.引擎平衡拉桿.....業務表示', '連力巨人和LED後照鏡 也沒有 ', '如果以目前1.8L規的單來看這無疑是超級大肥單但Nissan新車剛上市']\n",
      "出現負面詞彙的段落:['故還沒有配件', '不知道這個單如何可以需再多談些什麼配備嗎']\n",
      "---------------------------------------------\n",
      "positive keyword:['以上', '還有', '常見', '很多', '防水', '平衡', '表示', '也許', '巨人', '想要', '無疑', '超級', '優點', '謝謝']\n",
      "negative keyword:['還沒有', '不知道']\n",
      "=============================================\n",
      "整體輿論為正面\n",
      "出現正面詞彙的段落:['因為第一次買車 不知道能否下訂', '贈送配備', '煩請各位高手們給予指教 謝謝']\n",
      "出現負面詞彙的段落:['因為第一次買車 不知道能否下訂', '32.8 頭款']\n",
      "---------------------------------------------\n",
      "positive keyword:['第一次', '給予', '謝謝', '高手', '贈送']\n",
      "negative keyword:['不知道', '頭款']\n",
      "=============================================\n",
      "整體輿論為中性\n",
      "出現正面詞彙的段落:[]\n",
      "出現負面詞彙的段落:[]\n",
      "---------------------------------------------\n",
      "positive keyword:[]\n",
      "negative keyword:[]\n",
      "=============================================\n",
      "整體輿論為正面\n",
      "出現正面詞彙的段落:['附帶條件是保險一定要給他們保', '因為也是公司收入來源......', '8.力巨人排檔鎖', '謝謝', '11.防水托盤', '以上55.5 萬現金價 OR 30萬30期', '請問還有空間嗎', '3.海馬防水腳踏墊']\n",
      "出現負面詞彙的段落:['不用分期沒有比較便宜....']\n",
      "---------------------------------------------\n",
      "positive keyword:['防水', '巨人', '防水', '以上', '一定', '也是', '收入', '還有', '謝謝']\n",
      "negative keyword:['不用']\n",
      "=============================================\n",
      "整體輿論為正面\n",
      "出現正面詞彙的段落:['因為小弟過陣子會換到一個每天通勤上下班大概要70公里左右的路程去上班如果是以目前95市價來算的話一個月大概要4500左右油費', '還是本來就可以加了']\n",
      "出現負面詞彙的段落:['所以想加減省點油費但重點是加92對車子本身引擎會不會有啥麼影響']\n",
      "---------------------------------------------\n",
      "positive keyword:['大概', '還是']\n",
      "negative keyword:['不會']\n",
      "=============================================\n",
      "整體輿論為中性\n",
      "出現正面詞彙的段落:['還有那些地方要強化的']\n",
      "出現負面詞彙的段落:['小弟的331\\xa0\\xa0想掛渦輪不知道各位的看法如何']\n",
      "---------------------------------------------\n",
      "positive keyword:['還有']\n",
      "negative keyword:['不知道']\n",
      "=============================================\n",
      "整體輿論為正面\n",
      "出現正面詞彙的段落:['瓶身看了又看還是搞不太清楚問業代也是模糊不清只說是日本做的油品另外市面上所販售的機油除了中油製品外塑膠瓶裝的進口油應該都是台灣分裝應該是多了分裝費怎麼會比4L原裝鐵罐子的還要便宜', '以上請解惑...感恩']\n",
      "出現負面詞彙的段落:['瓶身看了又看還是搞不太清楚問業代也是模糊不清只說是日本做的油品另外市面上所販售的機油除了中油製品外塑膠瓶裝的進口油應該都是台灣分裝應該是多了分裝費怎麼會比4L原裝鐵罐子的還要便宜']\n",
      "---------------------------------------------\n",
      "positive keyword:['應該', '還是', '也是', '還要', '感恩', '以上']\n",
      "negative keyword:['怎麼會']\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "word_dict = {}\n",
    "emotion=[]\n",
    "path=\"E:\\\\PythonWin\\\\workspace\\\\dict\\\\emotionDict\\\\\"\n",
    "get_emotion_dict(path)\n",
    "jieba.load_userdict(\"C:\\\\Users\\\\BIG DATA\\\\Desktop\\\\test2.txt\")\n",
    "jieba.load_userdict(\"E:\\\\PythonWin\\\\workspace\\\\dict\\\\moedDict\\\\moedict--revised.txt\")\n",
    "positive = 0\n",
    "negative = 0\n",
    "pos = []\n",
    "neg = []\n",
    "posSen=[]\n",
    "negSen=[]\n",
    "IP = 'localhost'\n",
    "client = MongoClient(IP, 27017)\n",
    "# Select database\n",
    "db = client.test\n",
    "# Select collection\n",
    "collection = db.test\n",
    "datas = collection.find({\"brand\":\"Nissan\"})\n",
    "for idx , data in enumerate (datas):\n",
    "    if idx <10:\n",
    "        pos=[]\n",
    "        neg=[]\n",
    "        posSen=[]\n",
    "        negSen=[]\n",
    "        positive = 0\n",
    "        negative = 0\n",
    "        contents = cutSentence(data['content'])\n",
    "        for content in contents:\n",
    "            wdCuts = jieba.analyse.extract_tags(content)\n",
    "            for wdCut in wdCuts:\n",
    "                if wdCut in word_dict['positiveDict']:\n",
    "                    positive +=1\n",
    "                    pos.append(wdCut)\n",
    "                    posSen.append(content)\n",
    "                    posSen = list(set(posSen))\n",
    "                elif wdCut in word_dict['negativeDict']:\n",
    "                    negative +=1\n",
    "                    neg.append(wdCut)\n",
    "                    negSen.append(content)\n",
    "                    negSen = list(set(negSen))\n",
    "                else:None\n",
    "        if positive > negative:print(\"整體輿論為正面\")\n",
    "        elif positive < negative:print(\"整體輿論為負面\")\n",
    "        else:print(\"整體輿論為中性\")\n",
    "        print(\"出現正面詞彙的段落:{}\".format(posSen))\n",
    "        print(\"出現負面詞彙的段落:{}\".format(negSen))\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(\"positive keyword:{}\".format(pos))\n",
    "        print(\"negative keyword:{}\".format(neg))\n",
    "        print(\"=============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'signal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b33458c7ad6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msign\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'signal' is not defined"
     ]
    }
   ],
   "source": [
    "for sign in signal:\n",
    "    print(sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeff!!', '!', '\\\\xa5', '\\\\xa4', '\\\\xa3', '\\\\xa2', '\\\\xa1', '\\\\xa0', '%', '$', \"'\", '&', ')', '(', '+', '*', '-', '/', ':', '=', '<', '?', '>', '@', '\\\\xc2', '#', '\"', '[', ']', '\\\\\\\\', '_', '^', '`', '{', '}', '|', '~', '–', '!!', '?!', '??', '!?', '`', '``', \"''\", ',', ':', ';', '\"', \"'\", '<', '>', '*', '..', '...', '→', '？', '?', '，', '。', '；', '※', '～', '∕', '⊙', '＝', '﹦', '≠', '»', '×', '÷', '∀', '∇', '∥', '∩', '∪', '∴', '∼', '≒', '≡', '≦', '≧', '⋯', '■', '□', '▲', '△', '▼', '▽', '◆', '○', '◎', '●', '↑', '→', '☞', '↓', '←', '☜', '─', '═', '│', '┌', '╭', '┐', '╮', '└', '╰', '┘', '╯', '┬', '╧', '╱', '╳', '▁', '▂', '▃', '▄', '▅', '▆', '▇', '█', '®', '°', '・', '･', '‥', '〒', '☆', '★', '☺', '♞', '♪', '✌', '❤', '•', '︰', '※', '<br>', '</br>']\n"
     ]
    }
   ],
   "source": [
    "with open(\"E:\\\\PythonWin\\\\workspace\\\\dict\\\\uselessDict\\\\specialSignList.txt\",'r',encoding='utf-8') as frS:\n",
    "    cutWds=frS.read().strip().split('\\n')\n",
    "    cutlist = [cutWd for cutWd in cutWds if len(cutWd)>0]\n",
    "    print(cutlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emotion_dict(path):\n",
    "    emotionDict= ['negativeDict','positiveDict']\n",
    "    for emotionWd in emotionDict:\n",
    "        open_dict(path,emotionWd)   \n",
    "        # print(word_dict)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_dict(path,emotionWd):\n",
    "    with open(path+emotionWd+\".txt\",\"r\",encoding='utf-8') as fr:\n",
    "        data=fr.read().strip().split('\\n')\n",
    "        dict_list = [word for word in data if len(word)>0]\n",
    "        word_dict[emotionWd] = dict_list\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_brand_keyWord(column,word_dict):\n",
    "    data[column] = data[column].lower().replace('-', '_')\n",
    "    WdCuts = jieba.analyse.extract_tags(data[column])\n",
    "    for WdCut in WdCuts:\n",
    "        for key, word_list in word_dict.items():\n",
    "            if WdCut in word_list:\n",
    "                brand.append(key)\n",
    "    return brand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cutSentence(string):  ##放入原始文章路徑, 增加斷詞的list\n",
    "    #     text = codecs.open(text_path,\"r\",\"utf-8\")   #開檔 #讀取存成TXT檔的文字，讀入後統一轉成UTF-8格式\n",
    "    sentence = \"\"\n",
    "    textList = []\n",
    "\n",
    "#     print('\\n',type(string))\n",
    "    # print(string)\n",
    "    for word in string:\n",
    "        # print(word)\n",
    "        if word not in cutlist:  # 如果文字不是標點符號，就把字加到句子中\n",
    "            sentence += word\n",
    "            # print(sentence)\n",
    "        else:\n",
    "            textList.append(sentence)  # 如果遇到標點符號，把句子加到 text list中\n",
    "            sentence = \"\"\n",
    "            # print(textList)\n",
    "\n",
    "    return textList  # 傳回一個文字陣列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿絕對不會\n",
      "絕對不會原諒你\n",
      "絕對地\n",
      "絕對沒有\n",
      "給我注意一點\n",
      "給我注意點\n",
      "給美國顏色看\n",
      "善罷干休\n",
      "善變\n",
      "肅清異己\n",
      "脹大\n",
      "脾氣\n"
     ]
    }
   ],
   "source": [
    "with open(\"E:\\\\PythonWin\\\\workspace\\\\dict\\\\emotionDict\\\\test.txt\",'r',encoding = 'utf-8') as frB:\n",
    "    bad_emotions = frB.read().strip().split('\\n')\n",
    "    for bad_emotion in bad_emotions:\n",
    "        print(bad_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "解惑\n",
      "感恩\n",
      "以上\n",
      "[[0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(bad_emotions)\n",
    "\n",
    "word = vectorizer.get_feature_names() \n",
    "for w in wdCuts:\n",
    "    print (w)\n",
    "    \n",
    "print (X.transpose().toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "weight = tfidf.toarray()    \n",
    "print (weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(tfidf[0], tfidf).flatten()\n",
    "print (cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
