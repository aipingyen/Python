{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•´é«”è¼¿è«–ç‚ºè² é¢\n",
      "å‡ºç¾æ­£é¢è©å½™çš„æ®µè½:[]\n",
      "å‡ºç¾è² é¢è©å½™çš„æ®µè½:['åªå¯æƒœåªæœ‰æ—¥æœ¬æ‰çœ‹å¾—åˆ°']\n",
      "---------------------------------------------\n",
      "positive keyword:[]\n",
      "negative keyword:['å¯æƒœ']\n",
      "=============================================\n",
      "æ•´é«”è¼¿è«–ç‚ºä¸­æ€§\n",
      "å‡ºç¾æ­£é¢è©å½™çš„æ®µè½:['å¥½åƒæ—¥ç³»è»ŠåŒ…æ‹¬toyota', 'æ˜¯å¦é‚„æœ‰å…¶ä»–äººæœ‰è½èªªå‘¢']\n",
      "å‡ºç¾è² é¢è©å½™çš„æ®µè½:['é‚£æ˜å¹´è²·è»Šä¸å°±å°‘äº†5è¬ç©ºé–“', 'æœƒä¸æœƒå¤ªèª‡å¼µ']\n",
      "---------------------------------------------\n",
      "positive keyword:['é‚„æœ‰', 'è½èªª', 'å¥½åƒ']\n",
      "negative keyword:['ä¸æœƒ', 'èª‡å¼µ', 'ä¸å°±']\n",
      "=============================================\n",
      "æ•´é«”è¼¿è«–ç‚ºæ­£é¢\n",
      "å‡ºç¾æ­£é¢è©å½™çš„æ®µè½:['ğŸ˜€ä»Šå¤©å»å±•ç¤ºå ´å»çœ‹äº†å¯¦è»Šæœ‰é»å¤±æœ›ä»¥ä¸‹å¿ƒå¾—1.1.6Livina å¤–å½¢ å¤ªæ¥Šæ˜¥  æ„Ÿè¦ºèª æ„ä¸å¤   åªæœ‰é ‚ç´šæ‰æœ‰å¤–åŒ…2.1.6Livina 49.9è¬ è·Ÿ 1.6 TIIDA 49.9  æ¯”çš®æ¤… TIIDA å„ª  æ¯”ç©ºé–“ TIIDA å„ª  æ¯”çµ„è£  TIIDA å„ª    æ¯”è³ªæ„Ÿ TIIDA å‹  æ¯”æ²¹è€—  TIIDA å‹   æ¯”æŠ˜åƒ¹ TIIDA å„ª  ç´„3']\n",
      "å‡ºç¾è² é¢è©å½™çš„æ®µè½:['ğŸ˜€ä»Šå¤©å»å±•ç¤ºå ´å»çœ‹äº†å¯¦è»Šæœ‰é»å¤±æœ›ä»¥ä¸‹å¿ƒå¾—1.1.6Livina å¤–å½¢ å¤ªæ¥Šæ˜¥  æ„Ÿè¦ºèª æ„ä¸å¤   åªæœ‰é ‚ç´šæ‰æœ‰å¤–åŒ…2.1.6Livina 49.9è¬ è·Ÿ 1.6 TIIDA 49.9  æ¯”çš®æ¤… TIIDA å„ª  æ¯”ç©ºé–“ TIIDA å„ª  æ¯”çµ„è£  TIIDA å„ª    æ¯”è³ªæ„Ÿ TIIDA å‹  æ¯”æ²¹è€—  TIIDA å‹   æ¯”æŠ˜åƒ¹ TIIDA å„ª  ç´„3']\n",
      "---------------------------------------------\n",
      "positive keyword:['æ„Ÿè¦º', 'èª æ„']\n",
      "negative keyword:['ä¸å¤ ']\n",
      "=============================================\n",
      "æ•´é«”è¼¿è«–ç‚ºæ­£é¢\n",
      "å‡ºç¾æ­£é¢è©å½™çš„æ®µè½:['é€™å…©å€‹æ˜¯æˆ‘æ¯”è¼ƒæƒ³è¦çš„', '6.åŠ æ»¿æ²¹ä»¥ä¸Š63.9è¬', 'æ¿ä¸Šå¦‚æœæœ‰æ›´å„ªé»çš„å–®', 'ä¹Ÿè¨±éœ€ç­‰å» å•†é–‹æ¨¡.é‡èº«è¨‚åšå§', 'è¬è¬', 'ä¿éšª.å‹•ä¿.é ˜ç‰Œ å¯¦å ±å¯¦éŠ·é‚„æœ‰å¾ˆå¤šå¸¸è¦‹çš„é…å‚™ ä¾‹', 'é˜²æ°´æ‰˜ç›¤.é¿å…‰å¢Š.å¼•æ“å¹³è¡¡æ‹‰æ¡¿.....æ¥­å‹™è¡¨ç¤º', 'é€£åŠ›å·¨äººå’ŒLEDå¾Œç…§é¡ ä¹Ÿæ²’æœ‰ ', 'å¦‚æœä»¥ç›®å‰1.8Lè¦çš„å–®ä¾†çœ‹é€™ç„¡ç–‘æ˜¯è¶…ç´šå¤§è‚¥å–®ä½†Nissanæ–°è»Šå‰›ä¸Šå¸‚']\n",
      "å‡ºç¾è² é¢è©å½™çš„æ®µè½:['æ•…é‚„æ²’æœ‰é…ä»¶', 'ä¸çŸ¥é“é€™å€‹å–®å¦‚ä½•å¯ä»¥éœ€å†å¤šè«‡äº›ä»€éº¼é…å‚™å—']\n",
      "---------------------------------------------\n",
      "positive keyword:['ä»¥ä¸Š', 'é‚„æœ‰', 'å¸¸è¦‹', 'å¾ˆå¤š', 'é˜²æ°´', 'å¹³è¡¡', 'è¡¨ç¤º', 'ä¹Ÿè¨±', 'å·¨äºº', 'æƒ³è¦', 'ç„¡ç–‘', 'è¶…ç´š', 'å„ªé»', 'è¬è¬']\n",
      "negative keyword:['é‚„æ²’æœ‰', 'ä¸çŸ¥é“']\n",
      "=============================================\n",
      "æ•´é«”è¼¿è«–ç‚ºæ­£é¢\n",
      "å‡ºç¾æ­£é¢è©å½™çš„æ®µè½:['å› ç‚ºç¬¬ä¸€æ¬¡è²·è»Š ä¸çŸ¥é“èƒ½å¦ä¸‹è¨‚', 'è´ˆé€é…å‚™', 'ç…©è«‹å„ä½é«˜æ‰‹å€‘çµ¦äºˆæŒ‡æ•™ è¬è¬']\n",
      "å‡ºç¾è² é¢è©å½™çš„æ®µè½:['å› ç‚ºç¬¬ä¸€æ¬¡è²·è»Š ä¸çŸ¥é“èƒ½å¦ä¸‹è¨‚', '32.8 é ­æ¬¾']\n",
      "---------------------------------------------\n",
      "positive keyword:['ç¬¬ä¸€æ¬¡', 'çµ¦äºˆ', 'è¬è¬', 'é«˜æ‰‹', 'è´ˆé€']\n",
      "negative keyword:['ä¸çŸ¥é“', 'é ­æ¬¾']\n",
      "=============================================\n",
      "æ•´é«”è¼¿è«–ç‚ºä¸­æ€§\n",
      "å‡ºç¾æ­£é¢è©å½™çš„æ®µè½:[]\n",
      "å‡ºç¾è² é¢è©å½™çš„æ®µè½:[]\n",
      "---------------------------------------------\n",
      "positive keyword:[]\n",
      "negative keyword:[]\n",
      "=============================================\n",
      "æ•´é«”è¼¿è«–ç‚ºæ­£é¢\n",
      "å‡ºç¾æ­£é¢è©å½™çš„æ®µè½:['é™„å¸¶æ¢ä»¶æ˜¯ä¿éšªä¸€å®šè¦çµ¦ä»–å€‘ä¿', 'å› ç‚ºä¹Ÿæ˜¯å…¬å¸æ”¶å…¥ä¾†æº......', '8.åŠ›å·¨äººæ’æª”é–', 'è¬è¬', '11.é˜²æ°´æ‰˜ç›¤', 'ä»¥ä¸Š55.5 è¬ç¾é‡‘åƒ¹ OR 30è¬30æœŸ', 'è«‹å•é‚„æœ‰ç©ºé–“å—', '3.æµ·é¦¬é˜²æ°´è…³è¸å¢Š']\n",
      "å‡ºç¾è² é¢è©å½™çš„æ®µè½:['ä¸ç”¨åˆ†æœŸæ²’æœ‰æ¯”è¼ƒä¾¿å®œ....']\n",
      "---------------------------------------------\n",
      "positive keyword:['é˜²æ°´', 'å·¨äºº', 'é˜²æ°´', 'ä»¥ä¸Š', 'ä¸€å®š', 'ä¹Ÿæ˜¯', 'æ”¶å…¥', 'é‚„æœ‰', 'è¬è¬']\n",
      "negative keyword:['ä¸ç”¨']\n",
      "=============================================\n",
      "æ•´é«”è¼¿è«–ç‚ºæ­£é¢\n",
      "å‡ºç¾æ­£é¢è©å½™çš„æ®µè½:['å› ç‚ºå°å¼Ÿéé™£å­æœƒæ›åˆ°ä¸€å€‹æ¯å¤©é€šå‹¤ä¸Šä¸‹ç­å¤§æ¦‚è¦70å…¬é‡Œå·¦å³çš„è·¯ç¨‹å»ä¸Šç­å¦‚æœæ˜¯ä»¥ç›®å‰95å¸‚åƒ¹ä¾†ç®—çš„è©±ä¸€å€‹æœˆå¤§æ¦‚è¦4500å·¦å³æ²¹è²»', 'é‚„æ˜¯æœ¬ä¾†å°±å¯ä»¥åŠ äº†']\n",
      "å‡ºç¾è² é¢è©å½™çš„æ®µè½:['æ‰€ä»¥æƒ³åŠ æ¸›çœé»æ²¹è²»ä½†é‡é»æ˜¯åŠ 92å°è»Šå­æœ¬èº«å¼•æ“æœƒä¸æœƒæœ‰å•¥éº¼å½±éŸ¿']\n",
      "---------------------------------------------\n",
      "positive keyword:['å¤§æ¦‚', 'é‚„æ˜¯']\n",
      "negative keyword:['ä¸æœƒ']\n",
      "=============================================\n",
      "æ•´é«”è¼¿è«–ç‚ºä¸­æ€§\n",
      "å‡ºç¾æ­£é¢è©å½™çš„æ®µè½:['é‚„æœ‰é‚£äº›åœ°æ–¹è¦å¼·åŒ–çš„']\n",
      "å‡ºç¾è² é¢è©å½™çš„æ®µè½:['å°å¼Ÿçš„331\\xa0\\xa0æƒ³æ›æ¸¦è¼ªä¸çŸ¥é“å„ä½çš„çœ‹æ³•å¦‚ä½•']\n",
      "---------------------------------------------\n",
      "positive keyword:['é‚„æœ‰']\n",
      "negative keyword:['ä¸çŸ¥é“']\n",
      "=============================================\n",
      "æ•´é«”è¼¿è«–ç‚ºæ­£é¢\n",
      "å‡ºç¾æ­£é¢è©å½™çš„æ®µè½:['ç“¶èº«çœ‹äº†åˆçœ‹é‚„æ˜¯æä¸å¤ªæ¸…æ¥šå•æ¥­ä»£ä¹Ÿæ˜¯æ¨¡ç³Šä¸æ¸…åªèªªæ˜¯æ—¥æœ¬åšçš„æ²¹å“å¦å¤–å¸‚é¢ä¸Šæ‰€è²©å”®çš„æ©Ÿæ²¹é™¤äº†ä¸­æ²¹è£½å“å¤–å¡‘è† ç“¶è£çš„é€²å£æ²¹æ‡‰è©²éƒ½æ˜¯å°ç£åˆ†è£æ‡‰è©²æ˜¯å¤šäº†åˆ†è£è²»æ€éº¼æœƒæ¯”4LåŸè£éµç½å­çš„é‚„è¦ä¾¿å®œ', 'ä»¥ä¸Šè«‹è§£æƒ‘...æ„Ÿæ©']\n",
      "å‡ºç¾è² é¢è©å½™çš„æ®µè½:['ç“¶èº«çœ‹äº†åˆçœ‹é‚„æ˜¯æä¸å¤ªæ¸…æ¥šå•æ¥­ä»£ä¹Ÿæ˜¯æ¨¡ç³Šä¸æ¸…åªèªªæ˜¯æ—¥æœ¬åšçš„æ²¹å“å¦å¤–å¸‚é¢ä¸Šæ‰€è²©å”®çš„æ©Ÿæ²¹é™¤äº†ä¸­æ²¹è£½å“å¤–å¡‘è† ç“¶è£çš„é€²å£æ²¹æ‡‰è©²éƒ½æ˜¯å°ç£åˆ†è£æ‡‰è©²æ˜¯å¤šäº†åˆ†è£è²»æ€éº¼æœƒæ¯”4LåŸè£éµç½å­çš„é‚„è¦ä¾¿å®œ']\n",
      "---------------------------------------------\n",
      "positive keyword:['æ‡‰è©²', 'é‚„æ˜¯', 'ä¹Ÿæ˜¯', 'é‚„è¦', 'æ„Ÿæ©', 'ä»¥ä¸Š']\n",
      "negative keyword:['æ€éº¼æœƒ']\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "word_dict = {}\n",
    "emotion=[]\n",
    "path=\"E:\\\\PythonWin\\\\workspace\\\\dict\\\\emotionDict\\\\\"\n",
    "get_emotion_dict(path)\n",
    "jieba.load_userdict(\"C:\\\\Users\\\\BIG DATA\\\\Desktop\\\\test2.txt\")\n",
    "jieba.load_userdict(\"E:\\\\PythonWin\\\\workspace\\\\dict\\\\moedDict\\\\moedict--revised.txt\")\n",
    "positive = 0\n",
    "negative = 0\n",
    "pos = []\n",
    "neg = []\n",
    "posSen=[]\n",
    "negSen=[]\n",
    "IP = 'localhost'\n",
    "client = MongoClient(IP, 27017)\n",
    "# Select database\n",
    "db = client.test\n",
    "# Select collection\n",
    "collection = db.test\n",
    "datas = collection.find({\"brand\":\"Nissan\"})\n",
    "for idx , data in enumerate (datas):\n",
    "    if idx <10:\n",
    "        pos=[]\n",
    "        neg=[]\n",
    "        posSen=[]\n",
    "        negSen=[]\n",
    "        positive = 0\n",
    "        negative = 0\n",
    "        contents = cutSentence(data['content'])\n",
    "        for content in contents:\n",
    "            wdCuts = jieba.analyse.extract_tags(content)\n",
    "            for wdCut in wdCuts:\n",
    "                if wdCut in word_dict['positiveDict']:\n",
    "                    positive +=1\n",
    "                    pos.append(wdCut)\n",
    "                    posSen.append(content)\n",
    "                    posSen = list(set(posSen))\n",
    "                elif wdCut in word_dict['negativeDict']:\n",
    "                    negative +=1\n",
    "                    neg.append(wdCut)\n",
    "                    negSen.append(content)\n",
    "                    negSen = list(set(negSen))\n",
    "                else:None\n",
    "        if positive > negative:print(\"æ•´é«”è¼¿è«–ç‚ºæ­£é¢\")\n",
    "        elif positive < negative:print(\"æ•´é«”è¼¿è«–ç‚ºè² é¢\")\n",
    "        else:print(\"æ•´é«”è¼¿è«–ç‚ºä¸­æ€§\")\n",
    "        print(\"å‡ºç¾æ­£é¢è©å½™çš„æ®µè½:{}\".format(posSen))\n",
    "        print(\"å‡ºç¾è² é¢è©å½™çš„æ®µè½:{}\".format(negSen))\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(\"positive keyword:{}\".format(pos))\n",
    "        print(\"negative keyword:{}\".format(neg))\n",
    "        print(\"=============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'signal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b33458c7ad6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msign\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msignal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'signal' is not defined"
     ]
    }
   ],
   "source": [
    "for sign in signal:\n",
    "    print(sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeff!!', '!', '\\\\xa5', '\\\\xa4', '\\\\xa3', '\\\\xa2', '\\\\xa1', '\\\\xa0', '%', '$', \"'\", '&', ')', '(', '+', '*', '-', '/', ':', '=', '<', '?', '>', '@', '\\\\xc2', '#', '\"', '[', ']', '\\\\\\\\', '_', '^', '`', '{', '}', '|', '~', 'â€“', '!!', '?!', '??', '!?', '`', '``', \"''\", ',', ':', ';', '\"', \"'\", '<', '>', '*', '..', '...', 'â†’', 'ï¼Ÿ', '?', 'ï¼Œ', 'ã€‚', 'ï¼›', 'â€»', 'ï½', 'âˆ•', 'âŠ™', 'ï¼', 'ï¹¦', 'â‰ ', 'Â»', 'Ã—', 'Ã·', 'âˆ€', 'âˆ‡', 'âˆ¥', 'âˆ©', 'âˆª', 'âˆ´', 'âˆ¼', 'â‰’', 'â‰¡', 'â‰¦', 'â‰§', 'â‹¯', 'â– ', 'â–¡', 'â–²', 'â–³', 'â–¼', 'â–½', 'â—†', 'â—‹', 'â—', 'â—', 'â†‘', 'â†’', 'â˜', 'â†“', 'â†', 'â˜œ', 'â”€', 'â•', 'â”‚', 'â”Œ', 'â•­', 'â”', 'â•®', 'â””', 'â•°', 'â”˜', 'â•¯', 'â”¬', 'â•§', 'â•±', 'â•³', 'â–', 'â–‚', 'â–ƒ', 'â–„', 'â–…', 'â–†', 'â–‡', 'â–ˆ', 'Â®', 'Â°', 'ãƒ»', 'ï½¥', 'â€¥', 'ã€’', 'â˜†', 'â˜…', 'â˜º', 'â™', 'â™ª', 'âœŒ', 'â¤', 'â€¢', 'ï¸°', 'â€»', '<br>', '</br>']\n"
     ]
    }
   ],
   "source": [
    "with open(\"E:\\\\PythonWin\\\\workspace\\\\dict\\\\uselessDict\\\\specialSignList.txt\",'r',encoding='utf-8') as frS:\n",
    "    cutWds=frS.read().strip().split('\\n')\n",
    "    cutlist = [cutWd for cutWd in cutWds if len(cutWd)>0]\n",
    "    print(cutlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emotion_dict(path):\n",
    "    emotionDict= ['negativeDict','positiveDict']\n",
    "    for emotionWd in emotionDict:\n",
    "        open_dict(path,emotionWd)   \n",
    "        # print(word_dict)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_dict(path,emotionWd):\n",
    "    with open(path+emotionWd+\".txt\",\"r\",encoding='utf-8') as fr:\n",
    "        data=fr.read().strip().split('\\n')\n",
    "        dict_list = [word for word in data if len(word)>0]\n",
    "        word_dict[emotionWd] = dict_list\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_brand_keyWord(column,word_dict):\n",
    "    data[column] = data[column].lower().replace('-', '_')\n",
    "    WdCuts = jieba.analyse.extract_tags(data[column])\n",
    "    for WdCut in WdCuts:\n",
    "        for key, word_list in word_dict.items():\n",
    "            if WdCut in word_list:\n",
    "                brand.append(key)\n",
    "    return brand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cutSentence(string):  ##æ”¾å…¥åŸå§‹æ–‡ç« è·¯å¾‘, å¢åŠ æ–·è©çš„list\n",
    "    #     text = codecs.open(text_path,\"r\",\"utf-8\")   #é–‹æª” #è®€å–å­˜æˆTXTæª”çš„æ–‡å­—ï¼Œè®€å…¥å¾Œçµ±ä¸€è½‰æˆUTF-8æ ¼å¼\n",
    "    sentence = \"\"\n",
    "    textList = []\n",
    "\n",
    "#     print('\\n',type(string))\n",
    "    # print(string)\n",
    "    for word in string:\n",
    "        # print(word)\n",
    "        if word not in cutlist:  # å¦‚æœæ–‡å­—ä¸æ˜¯æ¨™é»ç¬¦è™Ÿï¼Œå°±æŠŠå­—åŠ åˆ°å¥å­ä¸­\n",
    "            sentence += word\n",
    "            # print(sentence)\n",
    "        else:\n",
    "            textList.append(sentence)  # å¦‚æœé‡åˆ°æ¨™é»ç¬¦è™Ÿï¼ŒæŠŠå¥å­åŠ åˆ° text listä¸­\n",
    "            sentence = \"\"\n",
    "            # print(textList)\n",
    "\n",
    "    return textList  # å‚³å›ä¸€å€‹æ–‡å­—é™£åˆ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿çµ•å°ä¸æœƒ\n",
      "çµ•å°ä¸æœƒåŸè«’ä½ \n",
      "çµ•å°åœ°\n",
      "çµ•å°æ²’æœ‰\n",
      "çµ¦æˆ‘æ³¨æ„ä¸€é»\n",
      "çµ¦æˆ‘æ³¨æ„é»\n",
      "çµ¦ç¾åœ‹é¡è‰²çœ‹\n",
      "å–„ç½·å¹²ä¼‘\n",
      "å–„è®Š\n",
      "è‚…æ¸…ç•°å·±\n",
      "è„¹å¤§\n",
      "è„¾æ°£\n"
     ]
    }
   ],
   "source": [
    "with open(\"E:\\\\PythonWin\\\\workspace\\\\dict\\\\emotionDict\\\\test.txt\",'r',encoding = 'utf-8') as frB:\n",
    "    bad_emotions = frB.read().strip().split('\\n')\n",
    "    for bad_emotion in bad_emotions:\n",
    "        print(bad_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "è§£æƒ‘\n",
      "æ„Ÿæ©\n",
      "ä»¥ä¸Š\n",
      "[[0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "vectorizer = CountVectorizer() \n",
    "X = vectorizer.fit_transform(bad_emotions)\n",
    "\n",
    "word = vectorizer.get_feature_names() \n",
    "for w in wdCuts:\n",
    "    print (w)\n",
    "    \n",
    "print (X.transpose().toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "weight = tfidf.toarray()    \n",
    "print (weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities = linear_kernel(tfidf[0], tfidf).flatten()\n",
    "print (cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
